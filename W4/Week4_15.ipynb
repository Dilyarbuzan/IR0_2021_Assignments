{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoekmachines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by\n",
    "\n",
    "__Name__|Job| en vrienden\n",
    "\n",
    "__Student id__ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* De meeste opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar zichtbare tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. De andere tests worden na inleveren toegevoegd aan die cell.\n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Zoekmachines\" data-toc-modified-id=\"Zoekmachines-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Zoekmachines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notebook-made-by\" data-toc-modified-id=\"Notebook-made-by-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Notebook made by</a></span></li><li><span><a href=\"#Toelichting\" data-toc-modified-id=\"Toelichting-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Toelichting</a></span></li><li><span><a href=\"#Voor-het-inleveren!\" data-toc-modified-id=\"Voor-het-inleveren!-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Voor het inleveren!</a></span></li></ul></li><li><span><a href=\"#Week-5\" data-toc-modified-id=\"Week-5-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Week 5</a></span></li><li><span><a href=\"#Evaluation-of-Document-Classification-(4pt)\" data-toc-modified-id=\"Evaluation-of-Document-Classification-(4pt)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evaluation of Document Classification (4pt)</a></span></li><li><span><a href=\"#Relevance-feedback\" data-toc-modified-id=\"Relevance-feedback-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Relevance feedback</a></span></li><li><span><a href=\"#Book-excercises-(Chapter-12)\" data-toc-modified-id=\"Book-excercises-(Chapter-12)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Book excercises (Chapter 12)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise-12.3\" data-toc-modified-id=\"Exercise-12.3-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Exercise 12.3</a></span></li><li><span><a href=\"#Exercise-12.4\" data-toc-modified-id=\"Exercise-12.4-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Exercise 12.4</a></span></li><li><span><a href=\"#Exercise-12.5\" data-toc-modified-id=\"Exercise-12.5-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Exercise 12.5</a></span></li><li><span><a href=\"#Exercise-12.6\" data-toc-modified-id=\"Exercise-12.6-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Exercise 12.6</a></span></li><li><span><a href=\"#Exercise-12.7\" data-toc-modified-id=\"Exercise-12.7-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Exercise 12.7</a></span></li><li><span><a href=\"#Exercise-12.8\" data-toc-modified-id=\"Exercise-12.8-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Exercise 12.8</a></span></li><li><span><a href=\"#Exercise-12.9\" data-toc-modified-id=\"Exercise-12.9-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Exercise 12.9</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d17b0c05ea230fe3efa6c4c730e5d3d",
     "grade": false,
     "grade_id": "cell-0dc239c271cc4348",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e5290f34dd822935cc9647c92ab52bf",
     "grade": false,
     "grade_id": "cell-676805319399a2c8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_almost_equal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e3596612b2e30e05a39a90efaa10213",
     "grade": false,
     "grade_id": "dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Evaluation of Document Classification (4pt)\n",
    "\n",
    "\n",
    "Stel je hebt een Naive Bayes Document classifier getrained voor 1 klasse $C$. Stel je hebt een test-collectie van 400 documenten. \n",
    "\n",
    "Beantwoord de volgende vragen in de cellen hieronder.\n",
    "\n",
    "1. De classifier heeft elk document in de collectie geklassificeerd. Wat betekent het als je een precisie hebt van 60%? Hoeveel documenten zijn dan onterecht als behorende tot klasse $C$ geklassificeerd?\n",
    "2. Stel dat 300 van de test documenten in klasse $C$ vallen? Welke precisie baseline hoort bij deze test-collectie? Welke recall baseline? \n",
    "3. Stel je classifier heeft een recall van 50%. Stel 100 van de 400 docs zitten in klasse $C$.  De classifier hoeft niet elk document te classificeren.   Geef exact   aan wat de precisie kan zijn voor deze classifier. Geef dus de onder en bovengrens voor de precisie. \n",
    "4. Stel 100 van de 400 docs zitten in klasse $C$. Welke baseline heb je voor _accurracy_?\n",
    "\n",
    "Met een classificatie baseline bedoelen we een classifier die helemaal niet naar de input kijkt maar alleen slim kiest op basis van de verdeling van de documenten over de klasse $C$.\n",
    "Denk bijvoorbeeld aan de vraag naar het geslacht van een willekeurige KI student. Wat zou je kiezen als je niks van die student weet? Wat is de recall en de precisie voor het geslacht \"jongen\" dan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8a69866354761dcfeae9fba9722c7f6",
     "grade": false,
     "grade_id": "dc1-a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deel vraag 1\n",
    "onterecht=400*0.4\n",
    "onterecht\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c97a721e428bdaad36ee594108d1faa",
     "grade": false,
     "grade_id": "dc1-b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# deel vraag 2\n",
    "\n",
    "precisie_baseline=0.75\n",
    "recall_baseline=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fa13ba3f2ed975b15b716015df7c6fe",
     "grade": false,
     "grade_id": "dc1-c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# deel vraag 3\n",
    "\n",
    "ondergrens=50/350\n",
    "bovengrens=1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4eed9c562e3859d4d1d1d60eaa07e52a",
     "grade": false,
     "grade_id": "dc1-d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# deel vraag 4\n",
    "\n",
    "accuracy_baseline=0.25\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b7587fb014c64d42d8f31b257726c6d",
     "grade": true,
     "grade_id": "dc1at",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0<=onterecht<= 400\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ad8925c99616c2b3d18a31ab8c40342",
     "grade": true,
     "grade_id": "dc1bt",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0<=precisie_baseline<=1\n",
    "assert 0<=recall_baseline<= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c841bad375c60f7c2b2ac9995f31797b",
     "grade": true,
     "grade_id": "dc1ct",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0<=ondergrens<=1\n",
    "assert 0<=bovengrens<= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d2b11cfc4d858aad02eb80c21501ba5",
     "grade": true,
     "grade_id": "dc1dt",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0<=accuracy_baseline<=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a97cf2e5bf3fd7c54cbe770212bc0657",
     "grade": false,
     "grade_id": "rf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Relevance feedback\n",
    "\n",
    "In deze vraag gaan we de SMART versie van Rochio relevance feedback (vergelijking 9.3 in het boek) implementeren in Python. We gaan uit van een collectie met een vocabulair van 100 termen. Elke query en elk document is gerepresenteerd als een vector van lengte 100 gevuld met getallen tussen 0 en 1. \n",
    "\n",
    "Maak de functies voor `centroid` en `modified_query` af. Alle data zit in numpy arrays, dus je kunt er heel makkelijk mee werken op een manier die je kent uit linaire algebra. \n",
    "\n",
    "De data bestaat uit \n",
    "* een query van 2 woorden\n",
    "* 8 relevante documenten en 3 niet relevante documenten\n",
    "\n",
    "Hint: beide functies kan je zonder for loops schrijven, zelfs comprehensies zijn niet nodig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e8f478507614519cafd568509bf9b62",
     "grade": false,
     "grade_id": "rfd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.    0.    0.    0.    0.    0.    0.    0.4   0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.111\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "q=np.zeros(100)\n",
    "q[8],q[47]= .4, .111\n",
    "Relevant_docs=np.array([np.random.rand(100) for _ in range(8)])\n",
    "NonRelevant_docs=np.array([np.random.rand(100) for _ in range(3)])\n",
    "\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1348f5c27f4e0a745a2dcd3183f1e9fd",
     "grade": false,
     "grade_id": "rfa",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.52132201 0.70069126 0.27419757 0.57354831 0.38947516 0.71669612\n",
      " 0.31776971 0.55482585 0.60450479 0.37523207 0.09744411 0.67102138\n",
      " 0.58456364 0.55950582 0.58387793 0.23835311 0.50477095 0.50497654\n",
      " 0.48378155 0.48132416 0.52644992 0.58749888 0.68403531 0.61795757\n",
      " 0.6748203  0.43548182 0.63915471 0.47942266 0.59616126 0.38950644\n",
      " 0.32436876 0.59053071 0.5265976  0.79024351 0.63572725 0.72305162\n",
      " 0.28526317 0.34024644 0.41458014 0.56053152 0.49057296 0.44824011\n",
      " 0.44179305 0.72544813 0.61764018 0.43425236 0.33043548 0.68543859\n",
      " 0.43245737 0.8129015  0.42227871 0.43241577 0.46994747 0.85037983\n",
      " 0.50982488 0.40014797 0.46598783 0.89697185 0.57071679 0.72538509\n",
      " 0.60889159 0.45146382 0.79597026 0.4272906  0.71180215 0.70185855\n",
      " 0.42415128 0.68901303 0.42618543 0.62805071 0.57672326 0.43656689\n",
      " 0.64344005 0.1853379  0.66388813 0.46936899 0.67194159 0.76289514\n",
      " 0.70962991 0.58701247 0.82979926 0.28675836 0.72150106 0.92517077\n",
      " 0.55954131 0.54844045 0.42020659 0.75680428 0.56112219 0.37206649\n",
      " 0.40089763 0.40486177 0.49813107 0.63412424 0.44390622 0.60410586\n",
      " 0.79388091 0.62338359 0.55563603 0.66706929]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.17997597, 0.21956993, 0.34903058, 0.12796806, 0.23455774,\n",
       "       0.33119833, 0.35412237, 0.24682393, 0.53459601, 0.30528279,\n",
       "       0.40440239, 0.14741105, 0.17038862, 0.27168216, 0.19628559,\n",
       "       0.17026786, 0.17141847, 0.32334579, 0.44065036, 0.14600346,\n",
       "       0.21058499, 0.06866206, 0.2150058 , 0.21976514, 0.18798542,\n",
       "       0.22258144, 0.31493458, 0.3607935 , 0.17130592, 0.16262657,\n",
       "       0.26194687, 0.17953815, 0.30473325, 0.10273686, 0.25332565,\n",
       "       0.32934567, 0.50636269, 0.27688374, 0.1365464 , 0.29577977,\n",
       "       0.29190651, 0.28464658, 0.34726087, 0.12323596, 0.14031255,\n",
       "       0.31276385, 0.21701068, 0.25237687, 0.20083884, 0.11033035,\n",
       "       0.23259055, 0.21633952, 0.37916172, 0.18461693, 0.2727421 ,\n",
       "       0.32589311, 0.25132895, 0.099936  , 0.16538335, 0.26086133,\n",
       "       0.10384361, 0.35097377, 0.10768911, 0.31836799, 0.25489358,\n",
       "       0.10441795, 0.21519355, 0.02498851, 0.40832135, 0.21033657,\n",
       "       0.34370207, 0.35796194, 0.25211788, 0.40671727, 0.17013141,\n",
       "       0.23418563, 0.26727712, 0.09642201, 0.2758539 , 0.17909664,\n",
       "       0.11093611, 0.41889213, 0.29335853, 0.2349459 , 0.1149311 ,\n",
       "       0.41730789, 0.26357784, 0.31090478, 0.22678608, 0.26539815,\n",
       "       0.32343021, 0.1362947 , 0.17689634, 0.27765816, 0.25236088,\n",
       "       0.23828073, 0.16529722, 0.42178096, 0.14941122, 0.24785366])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def centroid(D):\n",
    "    return sum(D) / len(D)\n",
    "\n",
    "# test\n",
    "print(centroid(NonRelevant_docs ))\n",
    "\n",
    "def modified_query(query,relevantdocs,nonrelevantdocs,alpha=1,beta=.75,gamma=.25):\n",
    "    return alpha * query + beta * centroid(relevantdocs) - gamma * centroid(nonrelevantdocs)\n",
    "\n",
    "# test\n",
    "\n",
    "modified_query(q,Relevant_docs,NonRelevant_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "485b6a0a176f70ca1f45ea71c27482f0",
     "grade": true,
     "grade_id": "rft",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(modified_query(q,Relevant_docs,NonRelevant_docs), np.ndarray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d7df694df05113e912d126b09b6c9f9",
     "grade": false,
     "grade_id": "cell-3e4e40c77ee90ff9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Book excercises (Chapter 12)\n",
    "In this section you will make the exercises from the book, which is freely available online as a PDF at [nlp.stanford.edu/IR-book/](http://nlp.stanford.edu/IR-book/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1a8191b77712fbbbba8bcc43b0f12aa",
     "grade": false,
     "grade_id": "cell-4aea9bac2279df92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.3\n",
    "What  is  the  likelihood  ratio  of  the  document  according  to M1 and M2 in  Example 12.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec381cab9b09a8525e49d64f8e0e52fc",
     "grade": false,
     "grade_id": "cell-1078c13a222d6f56",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1250"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_ratio = (0.01 * 0.03 * 0.04 * 0.01 * 0.02 * 0.005) / (0.0002 * 0.03 * 0.04 * 0.0001 * 0.04 * 0.01)\n",
    "\n",
    "round(likelihood_ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d0a6e1e7e203657de00571545478e5",
     "grade": true,
     "grade_id": "cell-d5b4ca4da32d2e9c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(likelihood_ratio) in [int, float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1383371c963ca801d5f55e7561f5fd29",
     "grade": false,
     "grade_id": "cell-52afc97c2c1affa7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.4\n",
    "No explicit STOP probability appeared in Example 12.2. Assuming that the STOP probability of each model is 0.1, does this change the likelihood ratio of a document according to the two models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No. That would mean we have to multiply both terms in the division by $(0.1^5 * 0.9)$. But these additional multiplications would cancel each other out when the division is computed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95ad84aff08a3bf094637a7fa4cf4929",
     "grade": false,
     "grade_id": "cell-88d84f5f3e274f8c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "125000"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_with_stop = (0.01 * 0.03 * 0.04 * 0.01 * 0.02 * 0.01 * 0.1**5 * 0.9) / (0.0002 * 0.03 * 0.04 * 0.0001 * 0.04 * 0.0002 * 0.1**5 * 0.9)\n",
    "\n",
    "round(likelihood_with_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dca3116592fd9f20f4b756597a2d8f7",
     "grade": true,
     "grade_id": "cell-77b507102f5e246e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(likelihood_with_stop) in [int, float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eca59fc4b785355d473ffbf2f02a6e0a",
     "grade": false,
     "grade_id": "cell-ea87bbc3ac13b9e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.5\n",
    "How might a language model be used in a spelling correction system? In particular, consider the case of context-sensitive spelling correction, and correcting incorrect usages of words, such as _their_ in _Are you their?_(See Section3.5(page65) for pointers to some literature on this topic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a754001af758ad6d705b494b6270e515",
     "grade": true,
     "grade_id": "cell-4bf9772cefb54700",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "To detect context-sensitive spelling errors, could use a language model to evaluate whether a series of words is plausible. With an N-gram, we have a model that gives the probability of a word given the N words before it. When we evaluate a piece of text with an N-gram by finding its likelihood, we may finds parts of the text that give some really low probabilities. This may indicate multiple things. But one thing it could mean, is that there has been made a context-sensitive spelling error at the point of those low probabilities or just before them. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1c75e9e816af655cf15dc6b7f35d962",
     "grade": false,
     "grade_id": "cell-e00a0be849b70c38",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.6\n",
    "Consider making a language model from the following training text:\n",
    "    the martian has landed on the latin pop sensation ricky martin\n",
    "    \n",
    "a. Under a MLE-estimated unigram probability model, what are P(the) and P(martian)?\n",
    "\n",
    "b. Under a MLE-estimated bigram model, what are P(sensation|pop) and P(pop|the)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c792d168b56cd1ad528dc03338b2659",
     "grade": false,
     "grade_id": "cell-ecc32bc65a9fbd20",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2, 1)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'the martian has landed on the latin pop sensation ricky martin'.split()\n",
    "# This is all without smoothing\n",
    "P_the = 2 / 11\n",
    "P_martian = 1 / 11\n",
    "P_sensation_pop = 1 / 1\n",
    "P_pop_the = 0 / 2\n",
    "\n",
    "P_the, P_martian, P_sensation_pop, P_pop_the\n",
    "len(text),text.count(\"the\"), text.count(\"martian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd5a37099e70c13fe6df48b0ca1b2660",
     "grade": true,
     "grade_id": "cell-4bf550a6e30c9cce",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(P_the) in [int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92e5d88c190cb84ea0a347d8c8f9e8a5",
     "grade": true,
     "grade_id": "cell-cf1055abc4db1355",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(P_martian) in [int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "466e24652822b1c8bcfce854ae0e6549",
     "grade": true,
     "grade_id": "cell-c7730c5c9cd22dc6",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(P_sensation_pop) in [int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1f6434fb2c2724841937b1aa4f4b374",
     "grade": true,
     "grade_id": "cell-e1b617addf2268f7",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(P_pop_the) in [int, float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbcb70d1aa96be87f4d80269bff18d4b",
     "grade": false,
     "grade_id": "cell-98f412455f0eb0f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.7\n",
    "Suppose we have  a collection that consists of the 4 documents given in the below table.\n",
    "\n",
    "| docID | Document text                              |\n",
    "|-------|--------------------------------------------|\n",
    "| 1     | click go the shears boys click click click |\n",
    "| 2     | click click                                |\n",
    "| 3     | metal here                                 |\n",
    "| 4     | metal shears click here                    |\n",
    "\n",
    "\n",
    "Build a  query likelihood language  model for  this  document  collection. Assume  amixture model between the documents and the collection, with both weighted at 0.5. Maximum likelihood estimation (mle) is used to estimate both as unigram models. Work out the model probabilities of the queries `click`, `shears`, and hence `click shears` for each document, and use those probabilities to rank the documents returned by eachquery.\n",
    "\n",
    "For each query give the probablities in a list, where the first value corresponds to Doc1, the second value to Doc2 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca7488ef866faf4e306cbd98ebe95f11",
     "grade": false,
     "grade_id": "cell-5a45af37333b7b02",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.46875, 0.71875, 0.21875, 0.34375],\n",
       " [0.125, 0.0625, 0.0625, 0.1875],\n",
       " [0.05859375, 0.044921875, 0.013671875, 0.064453125])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lam = 0.5\n",
    "P_click = (lam * np.array([4/8, 2/2, 0/2, 1/4]) + (1 - lam) * 7 / 16).tolist()\n",
    "P_shears = (lam * np.array([1/8, 0/2, 0/2, 1/4]) + (1 - lam) * 2 / 16).tolist()\n",
    "P_click_shears = (np.array(P_click) * np.array(P_shears)).tolist()\n",
    "\n",
    "P_click, P_shears, P_click_shears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3ca8e15075d461c7008ecde18dd8f9a",
     "grade": true,
     "grade_id": "cell-544c7fce7e22a413",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(P_click), list)\n",
    "assert_equal(len(P_click), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4009f778785669bd6c01e801870b964",
     "grade": true,
     "grade_id": "cell-4d667ae597154cac",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(P_shears), list)\n",
    "assert_equal(len(P_shears), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a41fe15b248bba122e8acb78fc3abba9",
     "grade": true,
     "grade_id": "cell-bfe5b75eb01e6fdb",
     "locked": true,
     "points": 0.34,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(P_click_shears), list)\n",
    "assert_equal(len(P_click_shears), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "231a0626dd1afef7e2e1daa644a56e04",
     "grade": false,
     "grade_id": "cell-85983f7d071c5c6b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.8\n",
    "Using the  calculations in Exercise 12.7 as inspiration or as examples where appropriate, write one sentence each  describing  the  treatment  that  the  model  in  Equation (12.10) gives to each of the following quantities. Include whether it is present in the model or not and whether the effect is raw or scaled.\n",
    "\n",
    "a. Term frequency in a document\n",
    "\n",
    "b. Collection frequency of a term\n",
    "\n",
    "c. Document frequency of a term\n",
    "\n",
    "d. Length normalization of a term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95c8f4a31feac14b5c00b80cc3febdf3",
     "grade": true,
     "grade_id": "cell-9dda7fcb647eefde",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "a. The term frequency in a document is used to estimate the likelihood of a term (of a query) given a document. To get that estimate, we divide the term frequency in a document by the length of the document. The estimate is scaled by lambda.\n",
    "\n",
    "b. The collection frequency is used to estimate the likelihood of a term (of a query) given the collection of documents. To get that estimate, we divide the collection frequency by the length of the whole collection. The estimate is scaled by 1 - lambda.\n",
    "\n",
    "c. The document frequency of a term is not used.\n",
    "\n",
    "d. Length normalization of a term is not used. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff6e9ec25195ccae74da865a895016bf",
     "grade": false,
     "grade_id": "cell-9e0982d74d47e325",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.9\n",
    "In the mixture model approach to the query likelihood model (Equation (12.12)), the probability estimate of a term is based on the term frequency of a word in a document, and the collection frequency of the word.  Doing this certainly guarantees that each term of a query (in the vocabulary) has a non-zero chance of being generated by each document. But it has a more subtle but important effect of implementing a form of term weighting, related to what we saw in Chapter 6. Explain how this works. In particular, include  in  your answer a concrete numeric example showing  this  termweighting at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cff87de7523183885137733ad936c531",
     "grade": true,
     "grade_id": "cell-b7c0df0f7263b34b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Suppose we have a query ('marx', 'amsterdam', 'logic'). In the upper limit of $\\lambda=1$ the query becomes a conjunction ('marx'$\\land$'amsterdam'$\\land$'logic'), so this will probably return no documents about the philosopher Karl Marx and only ones about Maarten Marx. For lower values of $\\lambda$, results that do not strictly contain all different terms of the query start getting non-zero scores. It thus becomes more and more likely that such results get returned relatively highly. So results about Karl Marx will probably get returned sometimes for the query ('marx', 'amsterdam', 'logic') given a lower $\\lambda$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
