{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoekmachines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by\n",
    "\n",
    "__Name__|Yavuz| en vrienden\n",
    "\n",
    "__Student id__ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* De meeste opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar zichtbare tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. De andere tests worden na inleveren toegevoegd aan die cell.\n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Zoekmachines\" data-toc-modified-id=\"Zoekmachines-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Zoekmachines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notebook-made-by\" data-toc-modified-id=\"Notebook-made-by-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Notebook made by</a></span></li><li><span><a href=\"#Toelichting\" data-toc-modified-id=\"Toelichting-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Toelichting</a></span></li><li><span><a href=\"#Voor-het-inleveren!\" data-toc-modified-id=\"Voor-het-inleveren!-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Voor het inleveren!</a></span></li></ul></li><li><span><a href=\"#Week-5\" data-toc-modified-id=\"Week-5-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Week 5</a></span></li><li><span><a href=\"#Evaluation-of-Document-Classification-(4pt)\" data-toc-modified-id=\"Evaluation-of-Document-Classification-(4pt)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evaluation of Document Classification (4pt)</a></span></li><li><span><a href=\"#Relevance-feedback\" data-toc-modified-id=\"Relevance-feedback-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Relevance feedback</a></span></li><li><span><a href=\"#Book-excercises-(Chapter-12)\" data-toc-modified-id=\"Book-excercises-(Chapter-12)-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>Book excercises (Chapter 12)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise-12.3\" data-toc-modified-id=\"Exercise-12.3-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;</span>Exercise 12.3</a></span></li><li><span><a href=\"#Exercise-12.4\" data-toc-modified-id=\"Exercise-12.4-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;</span>Exercise 12.4</a></span></li><li><span><a href=\"#Exercise-12.5\" data-toc-modified-id=\"Exercise-12.5-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;</span>Exercise 12.5</a></span></li><li><span><a href=\"#Exercise-12.6\" data-toc-modified-id=\"Exercise-12.6-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;</span>Exercise 12.6</a></span></li><li><span><a href=\"#Exercise-12.7\" data-toc-modified-id=\"Exercise-12.7-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;</span>Exercise 12.7</a></span></li><li><span><a href=\"#Exercise-12.8\" data-toc-modified-id=\"Exercise-12.8-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;</span>Exercise 12.8</a></span></li><li><span><a href=\"#Exercise-12.9\" data-toc-modified-id=\"Exercise-12.9-5.7\"><span class=\"toc-item-num\">5.7&nbsp;&nbsp;</span>Exercise 12.9</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1d17b0c05ea230fe3efa6c4c730e5d3d",
     "grade": false,
     "grade_id": "cell-0dc239c271cc4348",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Week 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e5290f34dd822935cc9647c92ab52bf",
     "grade": false,
     "grade_id": "cell-676805319399a2c8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_almost_equal\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1e3596612b2e30e05a39a90efaa10213",
     "grade": false,
     "grade_id": "dc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Evaluation of Document Classification (4pt)\n",
    "\n",
    "\n",
    "Stel je hebt een Naive Bayes Document classifier getrained voor 1 klasse $C$. Stel je hebt een test-collectie van 400 documenten. \n",
    "\n",
    "Beantwoord de volgende vragen in de cellen hieronder.\n",
    "\n",
    "1. De classifier heeft elk document in de collectie geklassificeerd. Wat betekent het als je een precisie hebt van 60%? Hoeveel documenten zijn dan onterecht als behorende tot klasse $C$ geklassificeerd?\n",
    "2. Stel dat 300 van de test documenten in klasse $C$ vallen? Welke precisie baseline hoort bij deze test-collectie? Welke recall baseline? \n",
    "3. Stel je classifier heeft een recall van 50%. Stel 100 van de 400 docs zitten in klasse $C$.  De classifier hoeft niet elk document te classificeren.   Geef exact   aan wat de precisie kan zijn voor deze classifier. Geef dus de onder en bovengrens voor de precisie. \n",
    "4. Stel 100 van de 400 docs zitten in klasse $C$. Welke baseline heb je voor _accurracy_?\n",
    "\n",
    "Met een classificatie baseline bedoelen we een classifier die helemaal niet naar de input kijkt maar alleen slim kiest op basis van de verdeling van de documenten over de klasse $C$.\n",
    "Denk bijvoorbeeld aan de vraag naar het geslacht van een willekeurige KI student. Wat zou je kiezen als je niks van die student weet? Wat is de recall en de precisie voor het geslacht \"jongen\" dan?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c8a69866354761dcfeae9fba9722c7f6",
     "grade": false,
     "grade_id": "dc1-a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "160"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# deel vraag 1\n",
    "onterecht=160 \n",
    "onterecht\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4c97a721e428bdaad36ee594108d1faa",
     "grade": false,
     "grade_id": "dc1-b",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# deel vraag 2\n",
    "\n",
    "precisie_baseline= 300/(300+100)  \n",
    "recall_baseline= 300/300\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3fa13ba3f2ed975b15b716015df7c6fe",
     "grade": false,
     "grade_id": "dc1-c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# deel vraag 3\n",
    "\n",
    "ondergrens= 50/350  \n",
    "bovengrens= 50/50\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4eed9c562e3859d4d1d1d60eaa07e52a",
     "grade": false,
     "grade_id": "dc1-d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# deel vraag 4\n",
    "\n",
    "accuracy_baseline= 100/400 \n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1b7587fb014c64d42d8f31b257726c6d",
     "grade": true,
     "grade_id": "dc1at",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0<=onterecht<= 400\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0ad8925c99616c2b3d18a31ab8c40342",
     "grade": true,
     "grade_id": "dc1bt",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0<=precisie_baseline<=1\n",
    "assert 0<=recall_baseline<= 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c841bad375c60f7c2b2ac9995f31797b",
     "grade": true,
     "grade_id": "dc1ct",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0<=ondergrens<=1\n",
    "assert 0<=bovengrens<= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d2b11cfc4d858aad02eb80c21501ba5",
     "grade": true,
     "grade_id": "dc1dt",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert 0<=accuracy_baseline<=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a97cf2e5bf3fd7c54cbe770212bc0657",
     "grade": false,
     "grade_id": "rf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Relevance feedback\n",
    "\n",
    "In deze vraag gaan we de SMART versie van Rochio relevance feedback (vergelijking 9.3 in het boek) implementeren in Python. We gaan uit van een collectie met een vocabulair van 100 termen. Elke query en elk document is gerepresenteerd als een vector van lengte 100 gevuld met getallen tussen 0 en 1. \n",
    "\n",
    "Maak de functies voor `centroid` en `modified_query` af. Alle data zit in numpy arrays, dus je kunt er heel makkelijk mee werken op een manier die je kent uit linaire algebra. \n",
    "\n",
    "De data bestaat uit \n",
    "* een query van 2 woorden\n",
    "* 8 relevante documenten en 3 niet relevante documenten\n",
    "\n",
    "Hint: beide functies kan je zonder for loops schrijven, zelfs comprehensies zijn niet nodig."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0e8f478507614519cafd568509bf9b62",
     "grade": false,
     "grade_id": "rfd",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.    0.    0.    0.    0.    0.    0.    0.    0.4   0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.111\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.    0.\n",
      " 0.    0.    0.    0.   ]\n"
     ]
    }
   ],
   "source": [
    "# data\n",
    "q=np.zeros(100)\n",
    "q[8],q[47]= .4, .111\n",
    "Relevant_docs=np.array([np.random.rand(100) for _ in range(8)])\n",
    "NonRelevant_docs=np.array([np.random.rand(100) for _ in range(3)])\n",
    "\n",
    "print(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1348f5c27f4e0a745a2dcd3183f1e9fd",
     "grade": false,
     "grade_id": "rfa",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49256593 0.377335   0.75264358 0.52698157 0.43630413 0.50001595\n",
      " 0.57982357 0.61417493 0.31582894 0.19965316 0.63600346 0.7564291\n",
      " 0.28266996 0.70999573 0.58286854 0.37276832 0.52951547 0.48395082\n",
      " 0.3594933  0.51885877 0.48671166 0.68216535 0.63971676 0.56495293\n",
      " 0.54086738 0.44056973 0.2750561  0.64358084 0.37243697 0.43705967\n",
      " 0.53165023 0.43832933 0.09116858 0.57105882 0.55825556 0.32368387\n",
      " 0.42856189 0.73452655 0.46638674 0.49571169 0.47878846 0.66905335\n",
      " 0.26898043 0.34370542 0.31340432 0.58580012 0.46174647 0.64575032\n",
      " 0.53200323 0.43779763 0.65550502 0.405358   0.43909324 0.27116357\n",
      " 0.70132172 0.42270077 0.52967565 0.44167516 0.61788067 0.45473587\n",
      " 0.67830547 0.57171111 0.23042287 0.35674676 0.50348467 0.77225952\n",
      " 0.55169418 0.85957393 0.60912212 0.35598885 0.35023798 0.26101275\n",
      " 0.27765988 0.5537294  0.48266865 0.2828379  0.58711163 0.77617705\n",
      " 0.26734608 0.59538957 0.65250128 0.25859651 0.21226681 0.48185703\n",
      " 0.37815282 0.35160134 0.41067851 0.37652129 0.45158356 0.78628304\n",
      " 0.81469687 0.42933923 0.39145447 0.58489801 0.63542573 0.58958045\n",
      " 0.7701715  0.38817467 0.32498698 0.34667308]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.33767342, 0.22043823, 0.22818909, 0.397968  , 0.2382666 ,\n",
       "       0.27006696, 0.18524984, 0.24736252, 0.51183699, 0.44115203,\n",
       "       0.25924854, 0.32941842, 0.28418756, 0.16061271, 0.34760316,\n",
       "       0.26421935, 0.27947976, 0.24504866, 0.39641541, 0.15203864,\n",
       "       0.31008044, 0.13152647, 0.22910412, 0.1338627 , 0.10457301,\n",
       "       0.29529124, 0.37308769, 0.16931539, 0.24574673, 0.36409032,\n",
       "       0.11059464, 0.17022032, 0.33883787, 0.27156946, 0.06922223,\n",
       "       0.38338441, 0.23139651, 0.09186728, 0.18624959, 0.33247564,\n",
       "       0.25934463, 0.23283788, 0.25253798, 0.2959328 , 0.36784051,\n",
       "       0.29742892, 0.23658498, 0.2998856 , 0.23113668, 0.27549312,\n",
       "       0.27299636, 0.37706551, 0.29568375, 0.43492799, 0.12528171,\n",
       "       0.33681815, 0.25442946, 0.3272988 , 0.2583233 , 0.14657316,\n",
       "       0.17799218, 0.30227917, 0.31950097, 0.32972101, 0.3656171 ,\n",
       "       0.13850843, 0.21558148, 0.12926875, 0.34214952, 0.20986672,\n",
       "       0.30039813, 0.3624495 , 0.29847567, 0.32270069, 0.23080526,\n",
       "       0.18118241, 0.22649546, 0.32758896, 0.34856731, 0.10579303,\n",
       "       0.3182235 , 0.37359678, 0.23547527, 0.16285134, 0.28387711,\n",
       "       0.17022723, 0.35106005, 0.39735534, 0.21497636, 0.10518291,\n",
       "       0.24910113, 0.17919619, 0.32466696, 0.28438788, 0.24633994,\n",
       "       0.14649337, 0.23338297, 0.36133438, 0.26677423, 0.20312897])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def centroid(D):\n",
    "    \n",
    "    return np.sum(D, axis = 0)/len(D)\n",
    "\n",
    "# test\n",
    "print(centroid(NonRelevant_docs ))\n",
    "\n",
    "def modified_query(query,relevantdocs,nonrelevantdocs,alpha=1,beta=.75,gamma=.25):\n",
    "    \n",
    "    return alpha * query + beta * centroid(relevantdocs) - gamma * centroid(nonrelevantdocs)\n",
    "\n",
    "# test\n",
    "\n",
    "modified_query(q,Relevant_docs,NonRelevant_docs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "485b6a0a176f70ca1f45ea71c27482f0",
     "grade": true,
     "grade_id": "rft",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(modified_query(q,Relevant_docs,NonRelevant_docs), np.ndarray)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9d7df694df05113e912d126b09b6c9f9",
     "grade": false,
     "grade_id": "cell-3e4e40c77ee90ff9",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Book excercises (Chapter 12)\n",
    "In this section you will make the exercises from the book, which is freely available online as a PDF at [nlp.stanford.edu/IR-book/](http://nlp.stanford.edu/IR-book/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d1a8191b77712fbbbba8bcc43b0f12aa",
     "grade": false,
     "grade_id": "cell-4aea9bac2279df92",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.3\n",
    "What  is  the  likelihood  ratio  of  the  document  according  to M1 and M2 in  Example 12.2?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ec381cab9b09a8525e49d64f8e0e52fc",
     "grade": false,
     "grade_id": "cell-1078c13a222d6f56",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.1308988302963465"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_ratio = float(np.log((0.01*0.03*0.04*0.01*0.02*0.04*0.005)/(0.0002*0.03*0.04*0.0001*0.04*0.04*0.01)))\n",
    "\n",
    "likelihood_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "30d0a6e1e7e203657de00571545478e5",
     "grade": true,
     "grade_id": "cell-d5b4ca4da32d2e9c",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(likelihood_ratio) in [int, float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1383371c963ca801d5f55e7561f5fd29",
     "grade": false,
     "grade_id": "cell-52afc97c2c1affa7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.4\n",
    "No explicit STOP probability appeared in Example 12.2. Assuming that the STOP probability of each model is 0.1, does this change the likelihood ratio of a document according to the two models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95ad84aff08a3bf094637a7fa4cf4929",
     "grade": false,
     "grade_id": "cell-88d84f5f3e274f8c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.1308988302963465"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "likelihood_with_stop = float(np.log((0.01*0.03*0.04*0.01*0.02*0.04*0.005*0.1)/(0.0002*0.03*0.04*0.0001*0.04*0.04*0.01*0.1)))\n",
    "\n",
    "likelihood_with_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6dca3116592fd9f20f4b756597a2d8f7",
     "grade": true,
     "grade_id": "cell-77b507102f5e246e",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(likelihood_with_stop) in [int, float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "eca59fc4b785355d473ffbf2f02a6e0a",
     "grade": false,
     "grade_id": "cell-ea87bbc3ac13b9e2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.5\n",
    "How might a language model be used in a spelling correction system? In particular, consider the case of context-sensitive spelling correction, and correcting incorrect usages of words, such as _their_ in _Are you their?_(See Section3.5(page65) for pointers to some literature on this topic.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a754001af758ad6d705b494b6270e515",
     "grade": true,
     "grade_id": "cell-4bf9772cefb54700",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "A language model can be used in a spelling correction system by viewing the misspelled queries as probabilistic corruptions of a correct query. Using a probabilistic model, the misspellings can be corrected by taking the most probable correct query that is the closest in the given context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e1c75e9e816af655cf15dc6b7f35d962",
     "grade": false,
     "grade_id": "cell-e00a0be849b70c38",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.6\n",
    "Consider making a language model from the following training text:\n",
    "    the martian has landed on the latin pop sensation ricky martin\n",
    "    \n",
    "a. Under a MLE-estimated unigram probability model, what are P(the) and P(martian)?\n",
    "\n",
    "b. Under a MLE-estimated bigram model, what are P(sensation|pop) and P(pop|the)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0c792d168b56cd1ad528dc03338b2659",
     "grade": false,
     "grade_id": "cell-ecc32bc65a9fbd20",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(11, 2, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = [\"the\", \"martian\", \"has\", \"landed\", \"on\", \"the\", \"latin\", \"pop\", \"sensation\", \"ricky\", \"martin\"]\n",
    "P_the =  2/11\n",
    "P_martian = 1/11\n",
    "P_sensation_pop = 1/1\n",
    "P_pop_the = 0/2\n",
    "\n",
    "P_the, P_martian, P_sensation_pop, P_pop_the\n",
    "len(text),text.count(\"the\"), text.count(\"martian\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fd5a37099e70c13fe6df48b0ca1b2660",
     "grade": true,
     "grade_id": "cell-4bf550a6e30c9cce",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(P_the) in [int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "92e5d88c190cb84ea0a347d8c8f9e8a5",
     "grade": true,
     "grade_id": "cell-cf1055abc4db1355",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(P_martian) in [int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "466e24652822b1c8bcfce854ae0e6549",
     "grade": true,
     "grade_id": "cell-c7730c5c9cd22dc6",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(P_sensation_pop) in [int, float]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a1f6434fb2c2724841937b1aa4f4b374",
     "grade": true,
     "grade_id": "cell-e1b617addf2268f7",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(P_pop_the) in [int, float]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "fbcb70d1aa96be87f4d80269bff18d4b",
     "grade": false,
     "grade_id": "cell-98f412455f0eb0f8",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.7\n",
    "Suppose we have  a collection that consists of the 4 documents given in the below table.\n",
    "\n",
    "| docID | Document text                              |\n",
    "|-------|--------------------------------------------|\n",
    "| 1     | click go the shears boys click click click |\n",
    "| 2     | click click                                |\n",
    "| 3     | metal here                                 |\n",
    "| 4     | metal shears click here                    |\n",
    "\n",
    "\n",
    "Build a  query likelihood language  model for  this  document  collection. Assume  amixture model between the documents and the collection, with both weighted at 0.5. Maximum likelihood estimation (mle) is used to estimate both as unigram models. Work out the model probabilities of the queries `click`, `shears`, and hence `click shears` for each document, and use those probabilities to rank the documents returned by eachquery.\n",
    "\n",
    "For each query give the probablities in a list, where the first value corresponds to Doc1, the second value to Doc2 etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca7488ef866faf4e306cbd98ebe95f11",
     "grade": false,
     "grade_id": "cell-5a45af37333b7b02",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([0.46875, 0.71875, 0.21875, 0.34375],\n",
       " [0.125, 0.0625, 0.0625, 0.1875],\n",
       " [0.05859375, 0.044921875, 0.013671875, 0.064453125])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_click = [(4/8) * 0.5 + (7/16) * 0.5, (2/2) * 0.5 + (7/16) * 0.5, (0/2) * 0.5 + (7/16) * 0.5, (1/4) * 0.5 + (7/16) * 0.5]\n",
    "P_shears = [(1/8) * 0.5 + (2/16) * 0.5, (0/2) * 0.5 + (2/16) * 0.5, (0/2) * 0.5 + (2/16) * 0.5, (1/4) * 0.5 + (2/16) * 0.5]\n",
    "P_click_shears = [(((4/8) * 0.5 + (7/16) * 0.5) * ((1/8) * 0.5 + (2/16) * 0.5)), (((2/2) * 0.5 + (7/16) * 0.5) * ((0/2) * 0.5 + (2/16) * 0.5)), (((0/2) * 0.5 + (7/16) * 0.5) * ((0/2) * 0.5 + (2/16) * 0.5)), (((1/4) * 0.5 + (7/16) * 0.5) * ((1/4) * 0.5 + (2/16) * 0.5))]\n",
    "# Final ranking 4 > 1 > 2 > 3\n",
    "P_click, P_shears, P_click_shears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3ca8e15075d461c7008ecde18dd8f9a",
     "grade": true,
     "grade_id": "cell-544c7fce7e22a413",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(P_click), list)\n",
    "assert_equal(len(P_click), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b4009f778785669bd6c01e801870b964",
     "grade": true,
     "grade_id": "cell-4d667ae597154cac",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(P_shears), list)\n",
    "assert_equal(len(P_shears), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a41fe15b248bba122e8acb78fc3abba9",
     "grade": true,
     "grade_id": "cell-bfe5b75eb01e6fdb",
     "locked": true,
     "points": 0.34,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(P_click_shears), list)\n",
    "assert_equal(len(P_click_shears), 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "231a0626dd1afef7e2e1daa644a56e04",
     "grade": false,
     "grade_id": "cell-85983f7d071c5c6b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.8\n",
    "Using the  calculations in Exercise 12.7 as inspiration or as examples where appropriate, write one sentence each  describing  the  treatment  that  the  model  in  Equation (12.10) gives to each of the following quantities. Include whether it is present in the model or not and whether the effect is raw or scaled.\n",
    "\n",
    "a. Term frequency in a document\n",
    "\n",
    "b. Collection frequency of a term\n",
    "\n",
    "c. Document frequency of a term\n",
    "\n",
    "d. Length normalization of a term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "95c8f4a31feac14b5c00b80cc3febdf3",
     "grade": true,
     "grade_id": "cell-9dda7fcb647eefde",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "a) present: This is used to calculate the probability of the term appearing in the document. It is calculated by diving the term frequency by the total amount of words in the document.\n",
    "\n",
    "b) present: This is used to calculate the probability of the term appearing in the collection. It is calculated by dividing the term frequency by the total amount of words in the collection.\n",
    "\n",
    "c) not present\n",
    "\n",
    "d) not present"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ff6e9ec25195ccae74da865a895016bf",
     "grade": false,
     "grade_id": "cell-9e0982d74d47e325",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 12.9\n",
    "In the mixture model approach to the query likelihood model (Equation (12.12)), the probability estimate of a term is based on the term frequency of a word in a document, and the collection frequency of the word.  Doing this certainly guarantees that each term of a query (in the vocabulary) has a non-zero chance of being generated by each document. But it has a more subtle but important effect of implementing a form of term weighting, related to what we saw in Chapter 6. Explain how this works. In particular, include  in  your answer a concrete numeric example showing  this  termweighting at work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cff87de7523183885137733ad936c531",
     "grade": true,
     "grade_id": "cell-b7c0df0f7263b34b",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "When a word often occurs in a collection and its documents, it loses its worth. Words that are more unique, have a higher weighting because they are more rare. The mixture model makes sure of this, by lowering the worth of words that have a high term frequency. This is because they don't have any meaningful addition to the query if every document is returned because they have this often-occuring word. \n",
    "\n",
    "An example of this is as the following: <br>\n",
    "p(The) = 0.9<br>\n",
    "p(Automobile) = 0.4<br>\n",
    "Original value of 'The automobile': 0.9 * 0.4 = 0.36<br>\n",
    "New value of 'The automobile' with weighting: (0.1 * 0.9) * ((1-0.1) * 0.4) = 0.0324"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
