{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoekmachines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by\n",
    "\n",
    "__Name__|Group| en vrienden\n",
    "\n",
    "__Student id__ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* De meeste opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar zichtbare tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. De andere tests worden na inleveren toegevoegd aan die cell.\n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Week-3\" data-toc-modified-id=\"Week-3-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 3</a></span></li><li><span><a href=\"#Questions-from-Chapter-8-in-MRS\" data-toc-modified-id=\"Questions-from-Chapter-8-in-MRS-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Questions from Chapter 8 in MRS</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#8.1\" data-toc-modified-id=\"8.1-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>8.1</a></span></li><li><span><a href=\"#8.2\" data-toc-modified-id=\"8.2-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>8.2</a></span></li><li><span><a href=\"#8.3\" data-toc-modified-id=\"8.3-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>8.3</a></span></li><li><span><a href=\"#8.4\" data-toc-modified-id=\"8.4-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>8.4</a></span></li><li><span><a href=\"#8.8\" data-toc-modified-id=\"8.8-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>8.8</a></span></li><li><span><a href=\"#8.8-extra\" data-toc-modified-id=\"8.8-extra-2.0.6\"><span class=\"toc-item-num\">2.0.6&nbsp;&nbsp;</span>8.8 extra</a></span></li><li><span><a href=\"#8.9\" data-toc-modified-id=\"8.9-2.0.7\"><span class=\"toc-item-num\">2.0.7&nbsp;&nbsp;</span>8.9</a></span></li><li><span><a href=\"#8.10\" data-toc-modified-id=\"8.10-2.0.8\"><span class=\"toc-item-num\">2.0.8&nbsp;&nbsp;</span>8.10</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Evaluation-measures\" data-toc-modified-id=\"Evaluation-measures-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Evaluation measures</a></span></li><li><span><a href=\"#Precision-recall-curve\" data-toc-modified-id=\"Precision-recall-curve-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Precision recall curve</a></span></li><li><span><a href=\"#Cohen's-Kappa\" data-toc-modified-id=\"Cohen's-Kappa-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Cohen's Kappa</a></span></li><li><span><a href=\"#10.1-Creating-a-test-set\" data-toc-modified-id=\"10.1-Creating-a-test-set-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>10.1 Creating a test set</a></span></li><li><span><a href=\"#10.2-Assess-search-engine-results\" data-toc-modified-id=\"10.2-Assess-search-engine-results-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>10.2 Assess search engine results</a></span></li></ul></li></ul></li><li><span><a href=\"#Programming-(Inverted-Index)\" data-toc-modified-id=\"Programming-(Inverted-Index)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Programming (Inverted Index)</a></span><ul class=\"toc-item\"><li><span><a href=\"#12-Positional-inverted-index\" data-toc-modified-id=\"12-Positional-inverted-index-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>12 Positional inverted index</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36d7e1f5bd8f35d837a0d92a79995ce9",
     "grade": false,
     "grade_id": "cell-0e1219abb9e62568",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af12c8051c04438a128342ea6f2cee92",
     "grade": false,
     "grade_id": "cell-738a659fa61ddb20",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_almost_equal\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "128135c854ecdb8bf8f0f29376bc4ab5",
     "grade": false,
     "grade_id": "cell-65656d2217b99b63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Questions from Chapter 8 in MRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1960a516c228db42ab3e6df06fa6f44",
     "grade": false,
     "grade_id": "cell-a65f837aecb68efb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.1\n",
    "An IR system returns 8 relevant documents, and 10 nonrelevant documents. There are a total of 20 relevant documents in the collection. What is the precision of the system on this search, and what is its recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b66b1d464e71d7356374e98832cb6cd0",
     "grade": false,
     "grade_id": "cell-80134f1fef5eef35",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.4444444444444444)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = 8/(8 + (20-8)) # TP / (TP + FN)\n",
    "precision = 8/(8+10) # TP / (TP + FP)\n",
    "\n",
    "recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b4d587ae3ea201f9ce4b2e05d2d2b31",
     "grade": true,
     "grade_id": "cell-27cb4abf121eb1fe",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(precision), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d5f6a701436594eb4c2f3cb14e9841a",
     "grade": true,
     "grade_id": "cell-399797a07036b377",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(recall), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c4ed0af0704a4a084dbf1f53113c05b",
     "grade": false,
     "grade_id": "cell-d2f88cf1c913bc0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.2\n",
    "The balanced F measure (a.k.a. $F_1$) is defined as the harmonic mean of precision and recall. What is the advantage of using the harmonic mean rather than “averaging” (using the arithmetic mean)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d30fd647abab1986d0ef62ae8ec83c18",
     "grade": true,
     "grade_id": "cell-8c15f2522a286d6f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "> By using the harmonic mean, both precision and recall need to have quite high values to get a decent F1-score. So it requires both to have higher values to make the F1-score rise. If you use an arithmetic mean, if one of the scores is high and the other is low, it can still get a decent average."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffac5ce81d6510d57a4b6654212936ba",
     "grade": false,
     "grade_id": "cell-eed045f5e6b3ee53",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.3\n",
    "Derive the equivalence between the two formulas for F measure shown in Equation (8.5), given that $\\alpha = 1/(\\beta^2 + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34d760e6602741ea5c8d9ec7f0dbf70f",
     "grade": true,
     "grade_id": "cell-53a0d302308961e4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "First replace $\\alpha$ in $\\dfrac{1}{\\alpha\\frac{1}{P} + (1-\\alpha)\\frac{1}{R}}$. \n",
    "\n",
    "$\\dfrac{1}{\\frac{1}{\\beta^2+1}\\cdot\\frac{1}{P} + (1-\\frac{1}{\\beta^2+1})\\cdot\\frac{1}{R}}$\n",
    "\n",
    "Look at the elements in the denominator. \n",
    "\n",
    "$\\dfrac{1}{\\beta^2+1}\\cdot\\dfrac{1}{P} = \\dfrac{1}{P(\\beta^2+1)}$\n",
    "\n",
    "$(1-\\dfrac{1}{\\beta^2+1})\\cdot\\dfrac{1}{R} = \\dfrac{1}{R} - \\dfrac{1}{R(\\beta^2+1)} = \\dfrac{\\beta^2+1}{R(\\beta^2+1)} - \\dfrac{1}{R(\\beta^2+1)} = \\dfrac{\\beta^2}{R(\\beta^2+1)}$\n",
    "\n",
    "$\\dfrac{1}{P(\\beta^2+1)} + \\dfrac{\\beta^2}{R(\\beta^2+1)} = \\dfrac{R}{PR(\\beta^2+1)} + \\dfrac{P\\beta^2}{PR(\\beta^2+1)} = \\dfrac{R + P\\beta^2}{PR(\\beta^2+1)}$\n",
    "\n",
    "Replace denominator with the simplified denominator.\n",
    "\n",
    "$\\dfrac{1}{\\dfrac{R + P\\beta^2}{PR(\\beta^2+1)}} = \\dfrac{PR(\\beta^2+1)}{R + P\\beta^2}$\n",
    "\n",
    "So it is correct that the two formulas for F measure are equal. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "870db5075de36f7db00aa1d0dd4a5391",
     "grade": false,
     "grade_id": "cell-2261bc4dd91291e7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.4\n",
    "What are the possible values for interpolated precision at a recall level of 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ac0f0165254c66e09b35b5135de4cc4",
     "grade": true,
     "grade_id": "cell-cb243cc0dd24310e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "> The interpolated precision at recall level of 0 is the maximum precision for any recall level higher than or equal to 0 and lower than 1.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7111842d2595c012a3cd661b5d1b7003",
     "grade": false,
     "grade_id": "cell-d7783437d76e7668",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.8\n",
    "Consider an information need for which there are 4 relevant documents in the collection. Contrast two systems run on this collection. Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result):\n",
    "\n",
    "| System   | R for relevant, N for nonrelevant |\n",
    "|----------|:--------------------|\n",
    "| System 1 | R N R N N N N N R R |\n",
    "| System 2 | N R N N R R R N N N |\n",
    "\n",
    "1. What is the MAP of each system? Which has a higher MAP?\n",
    "2. Does this result intuitively make sense? What does it say about what is important in getting a good MAP score?\n",
    "3. What is the R-precision of each system? (Does it rank the systems the same as MAP?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. System 1 has a higher MAP. \n",
    "\n",
    "> 2. This does make sense, since System 1 has higher P@k scores in the beginning, since it ranks those two relevant docs at k=1 and k=3. So, to get a good MAP score, relevant docs should be ranked as high as possible. \n",
    "\n",
    "> 3. r/|Rel| = r/4, since there are 4 relevant documents. Look at the top 4 positions in the ranked hitlist, and see how many are relevant (r). Doing this on System 1 and System 2 gives the same ranking. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b0a3edc87feea9822de168e236475db",
     "grade": false,
     "grade_id": "cell-c42b1a9a69260467",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.5166666666666666, 0.5, 0.25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P_System1 = (1/4) * (1 + 2/3 + 3/9 + 4/10)\n",
    "P_System2 = (1/4) * (1/2 + 2/5 + 3/6 + 4/6)\n",
    "\n",
    "MAP_System1 = 1/1 * P_System1 # There's only 1 query, so only one AP?\n",
    "MAP_System2 = 1/1 * P_System2 # Same here?\n",
    "\n",
    "R_P_System1 = 2/4\n",
    "R_P_System2 = 1/4 \n",
    "\n",
    "MAP_System1, MAP_System2, R_P_System1, R_P_System2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7068ae217f1658d8f06112e3e40fa774",
     "grade": true,
     "grade_id": "cell-2c0a902081d76545",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System1), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "633b3e964c4aa9a772bc873bf010aeb5",
     "grade": true,
     "grade_id": "cell-084f6e73ce2878f5",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System2), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bc2ffe084a1acc42ab121385a18f7da",
     "grade": true,
     "grade_id": "cell-9614d1b2e6815021",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(R_P_System1), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "858ea4d21f32f7db0eb4d7aec019f611",
     "grade": true,
     "grade_id": "cell-c8aefed6b2b8398e",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(R_P_System2), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "773a7d83f5998b44de6f0af2d35ed94f",
     "grade": false,
     "grade_id": "88v",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.8 extra\n",
    "\n",
    "1. On the same data is used in the previous exercise, what would the MAP be when there would be 10 relevant documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75df7a1552351d14bc2a622f20081d00",
     "grade": false,
     "grade_id": "88a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24, 0.20666666666666667)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_System1ex = (1/10) * (1 + 2/3 + 3/9 + 4/10) # replace 4 with 10\n",
    "MAP_System2ex = (1/10) * (1/2 + 2/5 + 3/6 + 4/6) # same here\n",
    " \n",
    "MAP_System1ex, MAP_System2ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "125540b05112f0e8ecea9458fa616f70",
     "grade": true,
     "grade_id": "88t",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System1ex), float)\n",
    "assert_equal(type(MAP_System2ex), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "145183e2b1ba270789d3a8f18cb14e85",
     "grade": false,
     "grade_id": "cell-1f877ea183d14013",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.9\n",
    "\n",
    "The following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned documents in a ranked list of 20 documents retrieved in response to a query from a collection of 10,000 documents. The top of the ranked list (the document the system thinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant documents. Assume that there are 8 relevant documents in total in the collection.\n",
    "\n",
    " > R R N N N $\\quad$ N N N R N $\\quad$ R N N N R $\\quad$ N N N N R\n",
    "\n",
    "1. What is the precision of the system on the top 20? (variable `precision`)\n",
    "2. What is the F1 on the top 20? (variable `F1`)\n",
    "3. What is the uninterpolated precision of the system at 25% recall? (variable `uninterpolated`)\n",
    "4. What is the interpolated precision at 33% recall? (variable `interpolated`)\n",
    "5. Assume that these 20 documents are the complete result set of the system. What is the AP for the query?<br/><br/>\n",
    "Assume, now, instead, that the system returned the entire 10,000 documents in a ranked list, and these are the first 20 results returned.<br/><br/>\n",
    "6. What is the largest possible AP that this system could have? (variable `largest_AP`)\n",
    "7. What is the smallest possible AP that this system could have? (variable `smallest_AP`)\n",
    "8. In a set of experiments, only the top 20 results are evaluated by hand. The result in (e) is used to approximate the range (f)–(g). For this example, how large (in absolute terms) can the error for the AP be by calculating (e) instead of (f) and (g) for this query? (variable `Error`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15b06a1b04586c07b10c9cda022f8c03",
     "grade": false,
     "grade_id": "cell-6ececd8d73bfc0ca",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3,\n",
       " 0.4285714285714285,\n",
       " 1.0,\n",
       " 0.36363636363636365,\n",
       " 0.4162878787878788,\n",
       " 0.5034090909090909,\n",
       " 0.4164753875387539,\n",
       " -0.00018750875087508723)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "precision = 6/20\n",
    "recall = 6 / 8\n",
    "F1 = 2 * ((precision * recall) / (precision + recall))\n",
    "\"\"\"\n",
    "If recall == 25%, then it retrieved 2 relevant documents.\n",
    "\n",
    "The book says that if the (k+1)th doc retrieved is nonrelevant\n",
    "then recall stays the same, but precision drops. This means\n",
    "that after the first two docs (which are relevant) only non-\n",
    "relevant docs are to come. Recall stays 25%, but precision \n",
    "will drop. Look at first two docs of ranked list and calculate\n",
    "the precision. \n",
    "\"\"\"\n",
    "uninterpolated = 2/2\n",
    "\"\"\"\n",
    "If recall == 33%, then at least 3 relevant docs should've been\n",
    "returned. At this recall level precision = 3/9. At the next \n",
    "recall level, after returning 4 relevant docs, the precision \n",
    "is 4/11, which is higher. This is also the highest possible \n",
    "precision for recall levels higher than 33%. \n",
    "\"\"\"\n",
    "interpolated = 4/11\n",
    "AP_query = (1/8) * (1/1 + 2/2 + 3/9 + 4/11 + 5/15 + 6/20)\n",
    "\"\"\"\n",
    "1/6 -> 1/8, since all relevant documents will be retrieved. \n",
    "Highest AP means that the 2 remaining relevant docs will be\n",
    "returned next. Smallest AP means that the 2 remaining relevant\n",
    "docs will be returned last. \n",
    "\"\"\"\n",
    "largest_AP = (1/8) * (1/1 + 2/2 + 3/9 + 4/11 + 5/15 + 6/20 + 7/21 + 8/22)\n",
    "smallest_AP = (1/8) * (1/1 + 2/2 + 3/9 + 4/11 + 5/15 + 6/20 + 7/9999 + 8/10000)\n",
    "\"\"\"\n",
    "Error can either be at most max(AP_query - largest_AP, AP_query - smallest_AP). \n",
    "\"\"\"\n",
    "Error = max(AP_query - largest_AP, AP_query - smallest_AP)\n",
    "\n",
    "precision, F1, uninterpolated, interpolated, AP_query, largest_AP, smallest_AP, Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "692448c0d136e60c5ced6e3abe8f4fd3",
     "grade": true,
     "grade_id": "cell-c2bb032e00c4dee1",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d4b52d8e67f79a97cf5bf0a5f0b59ed",
     "grade": true,
     "grade_id": "cell-464b3522aa196af7",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(F1) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "217d3651897af1c4b8fbee04ce310b18",
     "grade": true,
     "grade_id": "cell-ee2af88f21b18a7d",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(uninterpolated) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daf0f299ac6ecc5f7545a662cbd9e16a",
     "grade": true,
     "grade_id": "cell-ee2af88f21b18a7dex",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(interpolated) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5d6fe375a8f2dec0a947b37f066af0b",
     "grade": true,
     "grade_id": "cell-dd18f398ad9a11ea",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(AP_query) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d4a9d3e1f376f843680fb044f6f79b5",
     "grade": true,
     "grade_id": "cell-43dee696647831ac",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(largest_AP) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eeae02e405950f380a48bdecbdc7a06d",
     "grade": true,
     "grade_id": "cell-6efcd2c3d9ed173c",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(smallest_AP) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d437ef9bd15ea427fc681b643afb534",
     "grade": true,
     "grade_id": "cell-dcd93ad480d010ac",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(Error) in [float,int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3aef48cb18e14805cdb8213df5707eee",
     "grade": false,
     "grade_id": "cell-653e5c66190845a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.10\n",
    "Below is a table showing how two human judges rated the relevance of a set of 12 documents to a particular information need (0 = nonrelevant, 1 = relevant). Let us assume that you’ve written an IR system that for this query returns the set of documents {4, 5, 6, 7, 8}\n",
    "\n",
    "| docId | Judge 1 | Judge 2|\n",
    "|------:|:--------|:-------|\n",
    "| 1     | 0       | 0      |\n",
    "| 2     | 0       | 0      |\n",
    "| 3     | 1       | 1      |\n",
    "| 4     | 1       | 1      |\n",
    "| 5     | 1       | 0      |\n",
    "| 6     | 1       | 0      |\n",
    "| 7     | 1       | 0      |\n",
    "| 8     | 1       | 0      |\n",
    "| 9     | 0       | 1      |\n",
    "| 10    | 0       | 1      |\n",
    "| 11    | 0       | 1      |\n",
    "| 12    | 0       | 1      |\n",
    "\n",
    "1. Calculate the kappa measure between the two judges.\n",
    "2. Calculate precision, recall, and F1 of your system if a document is considered relevant only if the two judges agree.\n",
    "3. Calculate precision, recall, and F1 of your system if a document is considered relevant if either judge thinks it is relevant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb4c1b27e6776637331d142902b8ccac",
     "grade": false,
     "grade_id": "cell-72efecf88879db26",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.33333333333333337\n",
      "0.2 0.5 0.28571428571428575\n",
      "1.0 0.5 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "P_A = 4 / 12\n",
    "P_nonrelevant = P_relevant = (6 + 6) / (12 + 12)\n",
    "P_E = P_nonrelevant**2 + P_relevant**2\n",
    "kappa = (P_A - P_E)/(1- P_E)\n",
    "\n",
    "# if the two judges agree.\n",
    "\"\"\"\n",
    "doc 3 and 4 the judges agree\n",
    "\"\"\"\n",
    "precision1 =  1 / 5\n",
    "recall1 =  1 / 2\n",
    "F1score1 = 2 * ((precision1 * recall1) / (precision1 + recall1))\n",
    "\n",
    "# if either judge thinks it is relevant.\n",
    "\"\"\"\n",
    "10 docs either one judge thinks is relevant\n",
    "3, 4, 5, 6, 7, 8, 9, 10, 11 , 12\n",
    "and retrieved\n",
    "{4, 5, 6, 7, 8}\n",
    "\"\"\"\n",
    "precision2 = 5 / 5\n",
    "recall2 = 5 / 10\n",
    "F1score2 = 2 * ((precision2 * recall2) / (precision2 + recall2))\n",
    "\n",
    "\n",
    "print(kappa)\n",
    "print(precision1, recall1, F1score1)\n",
    "print(precision2, recall2, F1score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad86d4c9429ec0ee3fe6b13676b9c72c",
     "grade": true,
     "grade_id": "cell-d0b49ee6b738d816",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(kappa) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "627faf4e76f87e5cb92feb67972bb9fa",
     "grade": true,
     "grade_id": "cell-6c4430b58ad99642",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision1) in [float,int]\n",
    "assert type(recall1) in [float,int]\n",
    "assert type(F1score1) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df6af0266e84e0a9a1d839637e6bba72",
     "grade": true,
     "grade_id": "cell-b4ddeb8ad1122e7e",
     "locked": true,
     "points": 0.34,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision2) in [float,int]\n",
    "assert type(recall2) in [float,int]\n",
    "assert type(F1score2) in [float,int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "278dc547a0840b2625d16df9ef63fd62",
     "grade": false,
     "grade_id": "cell-db608e43be498940",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28898816090b94a56863e26251744777",
     "grade": false,
     "grade_id": "em-v",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Evaluation measures\n",
    "\n",
    "1. Define a function `Rprecision(ranked_list_of_results,list_of_relevant_objects)` which does what it says, it returns the R-precision given the input data.\n",
    "2. Define a function `AveragePrecision(ranked_list_of_results,list_of_relevant_objects)` which returns the average precision of this list of results given the list of relevant answers.\n",
    "\n",
    "Both functions of course come with one or two well chosen tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de18476927f4ff49054c47b24b947ed4",
     "grade": false,
     "grade_id": "em-a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3333333333333333, 0.4777777777777777)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here you can add helper functions if needed or handy\n",
    "\n",
    "def Rprecision(ranked_list_of_results,list_of_relevant_objects):\n",
    "    check = ranked_list_of_results[:len(list_of_relevant_objects)]\n",
    "    r = 0\n",
    "    for doc in check:\n",
    "        if doc in list_of_relevant_objects:\n",
    "            r += 1           \n",
    "    return r / len(list_of_relevant_objects)\n",
    "    \n",
    "    \n",
    "def AveragePrecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "    full_score = 0\n",
    "    for k in range(1, len(ranked_list_of_results) + 1):\n",
    "        score = 0\n",
    "        for result in ranked_list_of_results[:k]:\n",
    "            if result in list_of_relevant_objects:\n",
    "                score += 1 \n",
    "        full_score += score  / k\n",
    "    return full_score * (1/len(list_of_relevant_objects))\n",
    "    \n",
    "\n",
    "    \n",
    "# Hint Define a couple of good tests \n",
    "Rprecision([4,5,1,2,3], [1,2,3]), AveragePrecision([4,5,1,2,3], [1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9191f5774f4535d104d1fe42ae7b5db9",
     "grade": true,
     "grade_id": "em-t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Rprecision([1], [1]), float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d05f65deb46ae4ba034726c7805266c",
     "grade": true,
     "grade_id": "em-t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(AveragePrecision([1], [1]), float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcac3e11641ce8c14ae19db88da51fc4",
     "grade": false,
     "grade_id": "pr",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Precision recall curve\n",
    "\n",
    "We create the interpolated precision-recall curve as in Figure 8.2 in MRS for one topic. We use for this the file \n",
    "`qrels.robust2004.txt`.\n",
    "\n",
    "It is easy to read in your data using pandas. We give some code to get you started.\n",
    "\n",
    "Store your answer in the dict `PR` using the provided schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46211e4c53a958b9fba1b3e925b1e6a6",
     "grade": false,
     "grade_id": "prc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: topic 301 0.9931814503797296\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPIC</th>\n",
       "      <th>ITERATION</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>RELEVANCY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10082</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10319</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TOPIC  ITERATION  DOCUMENT_ID  RELEVANCY\n",
       "0    301          0  FBIS3-10082          1\n",
       "1    301          0  FBIS3-10169          0\n",
       "2    301          0  FBIS3-10243          1\n",
       "3    301          0  FBIS3-10319          0\n",
       "4    301          0  FBIS3-10397          1"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels= pd.read_csv('qrels.robust2004.txt', sep=' ', header=None, \n",
    "                   names=['TOPIC',      'ITERATION',      'DOCUMENT_ID',     'RELEVANCY'])# https://trec.nist.gov/data/robust/qrels.robust2004.txt\n",
    "\n",
    "\n",
    "# Create Figure8\n",
    "\n",
    "rankedlist= qrels[(qrels.TOPIC==301)].DOCUMENT_ID.unique()\n",
    "np.random.shuffle(rankedlist) # This causes that the PR curve is different all the time (and often quite weird)\n",
    "relevantdocs=set(qrels[(qrels.TOPIC==301)&(qrels.RELEVANCY==1)].DOCUMENT_ID.unique() )\n",
    "\n",
    "N= len(relevantdocs)\n",
    "Rlevels=[(r+1) /N for r in range(len(relevantdocs))] \n",
    "\n",
    "print('AP: topic 301', AveragePrecision(rankedlist,relevantdocs  ))\n",
    "\n",
    "qrels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01027c61279685324d810cc33e4b7e55",
     "grade": false,
     "grade_id": "pra",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Figure8.2 for topic 301\n",
    "\n",
    "# You must calculate the correct precison values for every given recall value here\n",
    "PR={'UninterpolatedP':{r:.5 for r in Rlevels}, 'InterpolatedP':{r:.6 for r in Rlevels}}\n",
    "\n",
    "for n in range(1, N + 1):\n",
    "    Rlevel = Rlevels[n - 1]\n",
    "    # TPs are docs that are in first n docs in rankedlist and relevantdocs\n",
    "    # Divide this by number of docs returned (= precision)\n",
    "    precision = len(set(rankedlist[:n]).intersection(relevantdocs)) / n\n",
    "    PR['UninterpolatedP'][Rlevel] = precision\n",
    "    \n",
    "    # Check if there is a higher recall level which gives a higher precision\n",
    "    for n2 in range(n + 1, N + 1):\n",
    "        if len(set(rankedlist[:n2]).intersection(relevantdocs)) / n2 > precision:\n",
    "            precision = len(set(rankedlist[:n2]).intersection(relevantdocs)) / n2\n",
    "            \n",
    "    # Highest precision is precision at this Rlevel\n",
    "    PR['InterpolatedP'][Rlevel] = precision\n",
    "    \n",
    "\n",
    "pd.DataFrame(PR).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "326d67024d102de958f6dab1f96d765f",
     "grade": true,
     "grade_id": "prt",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert set(PR.keys())=={'UninterpolatedP', 'InterpolatedP'}\n",
    "for K in PR:\n",
    "     assert isinstance(PR[K],dict)\n",
    "assert_equal(set(PR['UninterpolatedP'].keys()),set(Rlevels))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d409e7643408910bcf2f768c2df01899",
     "grade": false,
     "grade_id": "ck-v",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Cohen's Kappa\n",
    "\n",
    "Compute the chance $P(E)$ used in the calculation of Cohen's Kappa, using the pooled marginals method.\n",
    "\n",
    "Your data looks like that given in Exercise 8.10: An array of pairs with binary relevance judgements in which the first column contains the scores of the first judge, etc.\n",
    "\n",
    "Hint: `numpy` arrays have all kind of handy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4416d9b5b37301b3a183748753f90b7",
     "grade": false,
     "grade_id": "ck-a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.52"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# input data size of kappa must be like this\n",
    "data= np.array([[1,1],[1,0],[0,1],[1,1],[0,0]])\n",
    "\n",
    "def PE(data):\n",
    "    '''On input data, return the   P(E) (expected agreement).'''\n",
    "    flatten = data.flatten()\n",
    "\n",
    "    non_relevant = (len(flatten) - sum(flatten))/len(flatten)\n",
    "    relevant = sum(flatten)/len(flatten)\n",
    "    P_E = non_relevant**2 + relevant**2\n",
    "\n",
    "    return    P_E\n",
    "\n",
    "PE(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cf21d47e9b7070cf58716b8627b85aa",
     "grade": true,
     "grade_id": "ck-t",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(PE(data), float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "627ad98ff8f230481ad539fd36a2df43",
     "grade": false,
     "grade_id": "cell-891a3bbce2d771fa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 10.1 Creating a test set\n",
    "\n",
    "Create a test collection in order to evaluate your two search engines (cosine similarity and BM25) on the set of Shakespeare. Create five information needs, which you express as queries. Use multi term queries only. For each information need you specify\n",
    "\n",
    " * The information need in natural language\n",
    " * The search query\n",
    " * A description when a document is considered relevant and when it is not. This should be a very clear description. A person assessing relevance may only use this together with the information need to determine relevance of a document. In particular the judge should not see the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52040542d5f6c5c6fea8d35ad4a1ffb5",
     "grade": true,
     "grade_id": "cell-3fd2c07428004c67",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16f16b8c95fcffd6029b5cfde151fa75",
     "grade": false,
     "grade_id": "cell-1af8400eea1e3e8e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 10.2 Assess search engine results\n",
    "\n",
    "Assess for your two search engines, for all five information needs the first 10 results given back by the systems. Let another student do the same task as well.\n",
    "1. Using your own relevance judgments, compute for each system, the precision at 10 for each query, and the average P@10. Discuss the results.\n",
    "2. You have now two times at least 50 relevance judgements, and probably more. Compute the kappa measure between the two judges as in exercise 8.10, using pooled marginals as in Table 8.2. Discuss your results and try to explain them.\n",
    "3. Now compute P@10 for both systems again, once using the relevance judgments of the other assessor, once in which you say that a document is relevant if BOTH judges said it was relevant, and once in which you say that a document is relevant if AT LEAST ONE of the judges said it was relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "668cb299c87264fda9c0bb50078702d1",
     "grade": true,
     "grade_id": "cell-1f5104381710e6d4",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64b042a414ab4186ff36da8de10fc0ca",
     "grade": false,
     "grade_id": "cell-0828c47c6c841ab2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Programming (Inverted Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45265903cced267bded49289c1a33748",
     "grade": false,
     "grade_id": "cell-ee70e59824b4530f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def loadShakespeare():\n",
    "    if 'shaks200.zip' in os.listdir():\n",
    "        return 'shaks200.zip'\n",
    "    elif os.path.exists('../../data/Week3/'):\n",
    "        return '../../data/Week3/shaks200.zip'\n",
    "    elif os.path.exists('../../../data/Week3/'):\n",
    "        return '../../../data/Week3/shaks200.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a291330f776161fa4f004ba6e02d511",
     "grade": false,
     "grade_id": "cell-2cd22bbe0ed6e5b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 12 Positional inverted index\n",
    "Adjust your software making an inverted index for the Shakespeare collection: the output should be a positional index. A positional index is an index that also stores the position of the occurrences of a word in a document. The following figure shows an example of a positional index.\n",
    "\n",
    "<img src=\"http://nlp.stanford.edu/IR-book/html/htmledition/img121.png\" />\n",
    "\n",
    "Make the index of the form (defaultdicts are also allowed!):\n",
    "```\n",
    "{'caesar':   {'files': \n",
    "                 {'lll': {'list': [25350], 'total': 1},\n",
    "                  'm_for_m': {'list': [6773, 14734], 'total': 2},\n",
    "                  'm_wives': {'list': [3459], 'total': 1},\n",
    "                  'macbeth': {'list': [9256], 'total': 1},\n",
    "                  'othello': {'list': [11728], 'total': 1},\n",
    "                  'rich_ii': {'list': [22695], 'total': 1},\n",
    "                  'rich_iii': {'list': [16418, 16570, 30799, 30801], 'total': 4},\n",
    "                  'titus': {'list': [368], 'total': 1},\n",
    "                  ....\n",
    "                  },\n",
    "               'total': 604}....\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d6c8aae378a5c4105975e2845550b86",
     "grade": true,
     "grade_id": "cell-498f786fcde3299d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fb9e4f4cad348b89a16dc3aee03fe28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'files': defaultdict(<function __main__.positional_inverted_index.<locals>.<lambda>.<locals>.<lambda>()>,\n",
       "             {'a_and_c': {'list': [22543, 32757], 'total': 2},\n",
       "              'all_well': {'list': [9691, 23363], 'total': 2},\n",
       "              'as_you': {'list': [9217], 'total': 1},\n",
       "              'com_err': {'list': [6116, 6140, 6540, 6587, 7012, 8857, 13501],\n",
       "               'total': 7},\n",
       "              'coriolan': {'list': [9042, 9356, 24826], 'total': 3},\n",
       "              'cymbelin': {'list': [2552, 8344], 'total': 2},\n",
       "              'dream': {'list': [8827,\n",
       "                9035,\n",
       "                9927,\n",
       "                10096,\n",
       "                14705,\n",
       "                15212,\n",
       "                16522,\n",
       "                20165],\n",
       "               'total': 8},\n",
       "              'hamlet': {'list': [14113, 15952, 32824, 33049], 'total': 4},\n",
       "              'hen_iv_2': {'list': [6904, 9288], 'total': 2},\n",
       "              'hen_v': {'list': [11420, 18505, 18531], 'total': 3},\n",
       "              'j_caesar': {'list': [17764, 17812], 'total': 2},\n",
       "              'john': {'list': [4513, 4517], 'total': 2},\n",
       "              'lear': {'list': [7099, 7717], 'total': 2},\n",
       "              'lll': {'list': [7963, 7986, 25450, 25482], 'total': 4},\n",
       "              'm_for_m': {'list': [11784, 27758], 'total': 2},\n",
       "              'm_wives': {'list': [1818, 11545, 11549, 27962], 'total': 4},\n",
       "              'much_ado': {'list': [20932,\n",
       "                20937,\n",
       "                20964,\n",
       "                20975,\n",
       "                20991,\n",
       "                21108,\n",
       "                23777,\n",
       "                24270],\n",
       "               'total': 8},\n",
       "              'othello': {'list': [663, 10418], 'total': 2},\n",
       "              'rich_ii': {'list': [27751], 'total': 1},\n",
       "              't_night': {'list': [1943,\n",
       "                7419,\n",
       "                8755,\n",
       "                8973,\n",
       "                8976,\n",
       "                14412,\n",
       "                22314,\n",
       "                22327],\n",
       "               'total': 8},\n",
       "              'taming': {'list': [7262, 12968, 16207, 24619], 'total': 4},\n",
       "              'tempest': {'list': [21019], 'total': 1},\n",
       "              'timon': {'list': [2924, 12321, 18613, 18619, 18807],\n",
       "               'total': 5},\n",
       "              'titus': {'list': [16569], 'total': 1},\n",
       "              'troilus': {'list': [8660, 20601, 27938, 27946, 27962, 31774],\n",
       "               'total': 6},\n",
       "              'two_gent': {'list': [6359, 8852, 9046, 20173], 'total': 4}}),\n",
       " 'total': 90}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def positional_inverted_index(zipfolder):\n",
    "    # With zipfile we can read the file without opening the zip file\n",
    "    archive = zipfile.ZipFile(zipfolder, 'r')\n",
    "    namelist = [x for x in archive.namelist() if '.xml' in x]\n",
    "    positional_index = defaultdict(lambda: {'files': defaultdict(lambda: {'list': [], 'total': 0}), 'total': 0})\n",
    "\n",
    "    for infile in tqdm_notebook(namelist): # loop over each file\n",
    "        f = archive.open(infile)\n",
    "        soup = BeautifulSoup(f, 'xml')\n",
    "        # get just the text from the soup\n",
    "        text = soup.get_text()\n",
    "        # tokenize the text\n",
    "        for position, term in enumerate(nltk.word_tokenize(text)):\n",
    "            term = term.lower()\n",
    "            # only add alpha characters\n",
    "            if term.isalpha(): \n",
    "                positional_index[term]['total'] += 1\n",
    "                positional_index[term]['files'][infile[:-4]]['list'].append(position)\n",
    "                positional_index[term]['files'][infile[:-4]]['total'] += 1\n",
    "        \n",
    "\n",
    "    return positional_index\n",
    "\n",
    "positional_index = positional_inverted_index(loadShakespeare())\n",
    "positional_index['ass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6e21d8ffeb36779f0b33421a0797a57",
     "grade": true,
     "grade_id": "cell-29881806b7e5b184",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70d267e8519c4b7386baa0989131f413",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=37), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "positional_index = positional_inverted_index(loadShakespeare())\n",
    "assert type(positional_index) in [dict, defaultdict]\n",
    "assert_equal(set(positional_index['the'].keys()), {'total','files'})\n",
    "assert_equal(type(positional_index['the']['total']), int)\n",
    "assert_equal(set(positional_index['the']['files']['lll'].keys()), {'total','list'})\n",
    "assert_equal(type(positional_index['the']['files']['lll']['list']), list)\n",
    "assert_equal(type(positional_index['the']['files']['lll']['total']), int)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
