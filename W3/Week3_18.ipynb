{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zoekmachines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook made by\n",
    "\n",
    "__Name__: \n",
    "\n",
    "__Student id__ : "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toelichting\n",
    "\n",
    "* De meeste opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar zichtbare tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. De andere tests worden na inleveren toegevoegd aan die cell.\n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Week-3\" data-toc-modified-id=\"Week-3-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Week 3</a></span></li><li><span><a href=\"#Questions-from-Chapter-8-in-MRS\" data-toc-modified-id=\"Questions-from-Chapter-8-in-MRS-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Questions from Chapter 8 in MRS</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#8.1\" data-toc-modified-id=\"8.1-2.0.1\"><span class=\"toc-item-num\">2.0.1&nbsp;&nbsp;</span>8.1</a></span></li><li><span><a href=\"#8.2\" data-toc-modified-id=\"8.2-2.0.2\"><span class=\"toc-item-num\">2.0.2&nbsp;&nbsp;</span>8.2</a></span></li><li><span><a href=\"#8.3\" data-toc-modified-id=\"8.3-2.0.3\"><span class=\"toc-item-num\">2.0.3&nbsp;&nbsp;</span>8.3</a></span></li><li><span><a href=\"#8.4\" data-toc-modified-id=\"8.4-2.0.4\"><span class=\"toc-item-num\">2.0.4&nbsp;&nbsp;</span>8.4</a></span></li><li><span><a href=\"#8.8\" data-toc-modified-id=\"8.8-2.0.5\"><span class=\"toc-item-num\">2.0.5&nbsp;&nbsp;</span>8.8</a></span></li><li><span><a href=\"#8.8-extra\" data-toc-modified-id=\"8.8-extra-2.0.6\"><span class=\"toc-item-num\">2.0.6&nbsp;&nbsp;</span>8.8 extra</a></span></li><li><span><a href=\"#8.9\" data-toc-modified-id=\"8.9-2.0.7\"><span class=\"toc-item-num\">2.0.7&nbsp;&nbsp;</span>8.9</a></span></li><li><span><a href=\"#8.10\" data-toc-modified-id=\"8.10-2.0.8\"><span class=\"toc-item-num\">2.0.8&nbsp;&nbsp;</span>8.10</a></span></li></ul></li></ul></li><li><span><a href=\"#Evaluation\" data-toc-modified-id=\"Evaluation-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Evaluation</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Evaluation-measures\" data-toc-modified-id=\"Evaluation-measures-3.0.1\"><span class=\"toc-item-num\">3.0.1&nbsp;&nbsp;</span>Evaluation measures</a></span></li><li><span><a href=\"#Precision-recall-curve\" data-toc-modified-id=\"Precision-recall-curve-3.0.2\"><span class=\"toc-item-num\">3.0.2&nbsp;&nbsp;</span>Precision recall curve</a></span></li><li><span><a href=\"#Cohen's-Kappa\" data-toc-modified-id=\"Cohen's-Kappa-3.0.3\"><span class=\"toc-item-num\">3.0.3&nbsp;&nbsp;</span>Cohen's Kappa</a></span></li><li><span><a href=\"#10.1-Creating-a-test-set\" data-toc-modified-id=\"10.1-Creating-a-test-set-3.0.4\"><span class=\"toc-item-num\">3.0.4&nbsp;&nbsp;</span>10.1 Creating a test set</a></span></li><li><span><a href=\"#10.2-Assess-search-engine-results\" data-toc-modified-id=\"10.2-Assess-search-engine-results-3.0.5\"><span class=\"toc-item-num\">3.0.5&nbsp;&nbsp;</span>10.2 Assess search engine results</a></span></li></ul></li></ul></li><li><span><a href=\"#Programming-(Inverted-Index)\" data-toc-modified-id=\"Programming-(Inverted-Index)-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Programming (Inverted Index)</a></span><ul class=\"toc-item\"><li><span><a href=\"#12-Positional-inverted-index\" data-toc-modified-id=\"12-Positional-inverted-index-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>12 Positional inverted index</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "36d7e1f5bd8f35d837a0d92a79995ce9",
     "grade": false,
     "grade_id": "cell-0e1219abb9e62568",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Week 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af12c8051c04438a128342ea6f2cee92",
     "grade": false,
     "grade_id": "cell-738a659fa61ddb20",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from nose.tools import assert_equal, assert_almost_equal\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.request import urlopen\n",
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import re\n",
    "import zipfile\n",
    "import math\n",
    "import numpy as np\n",
    "from tqdm import tqdm_notebook\n",
    "import nltk\n",
    "from collections import Counter, defaultdict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "128135c854ecdb8bf8f0f29376bc4ab5",
     "grade": false,
     "grade_id": "cell-65656d2217b99b63",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Questions from Chapter 8 in MRS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1960a516c228db42ab3e6df06fa6f44",
     "grade": false,
     "grade_id": "cell-a65f837aecb68efb",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.1\n",
    "An IR system returns 8 relevant documents, and 10 nonrelevant documents. There are a total of 20 relevant documents in the collection. What is the precision of the system on this search, and what is its recall?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b66b1d464e71d7356374e98832cb6cd0",
     "grade": false,
     "grade_id": "cell-80134f1fef5eef35",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.4, 0.4444444444444444)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall = 8/20 #replace with your answer\n",
    "precision = 8/(8+10) #replace with your answer\n",
    "\n",
    "recall, precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b4d587ae3ea201f9ce4b2e05d2d2b31",
     "grade": true,
     "grade_id": "cell-27cb4abf121eb1fe",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(precision), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d5f6a701436594eb4c2f3cb14e9841a",
     "grade": true,
     "grade_id": "cell-399797a07036b377",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(recall), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3c4ed0af0704a4a084dbf1f53113c05b",
     "grade": false,
     "grade_id": "cell-d2f88cf1c913bc0d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.2\n",
    "The balanced F measure (a.k.a. $F_1$) is defined as the harmonic mean of precision and recall. What is the advantage of using the harmonic mean rather than “averaging” (using the arithmetic mean)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d30fd647abab1986d0ef62ae8ec83c18",
     "grade": true,
     "grade_id": "cell-8c15f2522a286d6f",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Omdat je bij een gemiddelde nogsteeds 50% score kan behalen door alle documenten terug te geven. Immers is de recall dan 100% en de precisie bijna 0% -> 50% score bij een gemiddelde.\n",
    "\n",
    "Door de harmonic mean te gebruiken kan er gekozen worden dat recall of precision zwaarder weegt en zal een zoekmachine die alle documenten terug geeft zwaarder worden afgestraft."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ffac5ce81d6510d57a4b6654212936ba",
     "grade": false,
     "grade_id": "cell-eed045f5e6b3ee53",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.3\n",
    "Derive the equivalence between the two formulas for F measure shown in Equation (8.5), given that $\\alpha = 1/(\\beta^2 + 1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "34d760e6602741ea5c8d9ec7f0dbf70f",
     "grade": true,
     "grade_id": "cell-53a0d302308961e4",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "$$F = \\frac{1}{\\alpha \\frac{1}{P} + (1-\\alpha) \\frac{1}{R}} = $$\n",
    "$$\\frac{1}{\\frac{1}{\\beta^2 + 1} \\frac{1}{P} + (1-\\frac{1}{\\beta^2 + 1}) \\frac{1}{R}} = $$\n",
    "$$\\frac{1}{\\frac{1}{P\\beta^2 + P} + \\frac{1}{R}-\\frac{1}{R\\beta^2 + R}} = $$\n",
    "$$\\frac{1}{\\frac{1}{P\\beta^2 + P} + \\frac{\\beta^2 + 1}{\\beta^2 + 1} \\frac{1}{R}-\\frac{1}{R\\beta^2 + R}} = $$\n",
    "$$\\frac{1}{\\frac{1}{P\\beta^2 + P} + \\frac{\\beta^2 + 1}{R\\beta^2 + R}-\\frac{1}{R\\beta^2 + R}} = $$\n",
    "$$\\frac{1}{\\frac{1}{P\\beta^2 + P} + \\frac{\\beta^2}{R\\beta^2 + R}} = $$\n",
    "$$\\frac{1}{\\frac{R}{RP\\beta^2 + RP} + \\frac{P\\beta^2}{RP\\beta^2 + RP}} = $$\n",
    "$$\\frac{1}{\\frac{R + P\\beta^2}{RP\\beta^2 + RP}} = $$\n",
    "$$\\frac{RP\\beta^2 + RP}{R + P\\beta^2} = $$\n",
    "$$\\frac{(\\beta^2 + 1)RP}{\\beta^2P + R} = $$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "870db5075de36f7db00aa1d0dd4a5391",
     "grade": false,
     "grade_id": "cell-2261bc4dd91291e7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.4\n",
    "What are the possible values for interpolated precision at a recall level of 0?"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5ac0f0165254c66e09b35b5135de4cc4",
     "grade": true,
     "grade_id": "cell-cb243cc0dd24310e",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "Recall is 1. Zee image below for formula:\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7111842d2595c012a3cd661b5d1b7003",
     "grade": false,
     "grade_id": "cell-d7783437d76e7668",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.8\n",
    "Consider an information need for which there are 4 relevant documents in the collection. Contrast two systems run on this collection. Their top 10 results are judged for relevance as follows (the leftmost item is the top ranked search result):\n",
    "\n",
    "| System   | R for relevant, N for nonrelevant |\n",
    "|----------|:--------------------|\n",
    "| System 1 | R N R N N N N N R R |\n",
    "| System 2 | N R N N R R R N N N |\n",
    "\n",
    "1. What is the MAP of each system? Which has a higher MAP?\n",
    "2. Does this result intuitively make sense? What does it say about what is important in getting a good MAP score?\n",
    "3. What is the R-precision of each system? (Does it rank the systems the same as MAP?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|System   |           |\n",
    "|---------|           |\n",
    "|System1  | 1010000011|\n",
    "|System 2 | 0100111000|\n",
    "\n",
    "Precision at K is precisie over de lijst afgekapt op lengte K\n",
    "\n",
    "$$\\text{R-precision} = \\frac{\\text{Relevanten documenten die in de eerste n zijn terug gegeven}}{n}$$\n",
    "Waarbij: n = Hoeveel relevanten in dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5b0a3edc87feea9822de168e236475db",
     "grade": false,
     "grade_id": "cell-c42b1a9a69260467",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.6, 0.4928571428571428, 0.5, 0.25)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def calcMAP(string, totalRelevant):\n",
    "    result = 0\n",
    "    relevant = 0\n",
    "    \n",
    "    for i,char in enumerate(string):\n",
    "        if char == \"1\":\n",
    "            relevant += 1\n",
    "            result += relevant / (i + 1)\n",
    "            \n",
    "    return result / totalRelevant\n",
    "\n",
    "MAP_System1 = calcMAP(\"1010000011\", 4)  # replace with your answer\n",
    "MAP_System2 = calcMAP(\"0100111000\", 4) # replace with your answer\n",
    "R_P_System1 = 2/4 # replace with your answer\n",
    "R_P_System2 = 1/4 # replace with your answer\n",
    "\n",
    "MAP_System1, MAP_System2, R_P_System1, R_P_System2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7068ae217f1658d8f06112e3e40fa774",
     "grade": true,
     "grade_id": "cell-2c0a902081d76545",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System1), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "633b3e964c4aa9a772bc873bf010aeb5",
     "grade": true,
     "grade_id": "cell-084f6e73ce2878f5",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System2), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5bc2ffe084a1acc42ab121385a18f7da",
     "grade": true,
     "grade_id": "cell-9614d1b2e6815021",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(R_P_System1), float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "858ea4d21f32f7db0eb4d7aec019f611",
     "grade": true,
     "grade_id": "cell-c8aefed6b2b8398e",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(R_P_System2), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "773a7d83f5998b44de6f0af2d35ed94f",
     "grade": false,
     "grade_id": "88v",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.8 extra\n",
    "\n",
    "1. On the same data is used in the previous exercise, what would the MAP be when there would be 10 relevant documents?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "75df7a1552351d14bc2a622f20081d00",
     "grade": false,
     "grade_id": "88a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.24, 0.19714285714285712)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAP_System1ex = calcMAP(\"1010000011\", 10) # replace with your answer\n",
    "MAP_System2ex = calcMAP(\"0100111000\", 10) # replace with your answer\n",
    " \n",
    "MAP_System1ex, MAP_System2ex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "125540b05112f0e8ecea9458fa616f70",
     "grade": true,
     "grade_id": "88t",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(MAP_System1ex), float)\n",
    "assert_equal(type(MAP_System2ex), float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "145183e2b1ba270789d3a8f18cb14e85",
     "grade": false,
     "grade_id": "cell-1f877ea183d14013",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.9\n",
    "\n",
    "The following list of Rs and Ns represents relevant (R) and nonrelevant (N) returned documents in a ranked list of 20 documents retrieved in response to a query from a collection of 10,000 documents. The top of the ranked list (the document the system thinks is most likely to be relevant) is on the left of the list. This list shows 6 relevant documents. Assume that there are 8 relevant documents in total in the collection.\n",
    "\n",
    " > R R N N N $\\quad$ N N N R N $\\quad$ R N N N R $\\quad$ N N N N R\n",
    "\n",
    "1. What is the precision of the system on the top 20? (variable `precision`)\n",
    "2. What is the F1 on the top 20? (variable `F1`)\n",
    "3. What is the uninterpolated precision of the system at 25% recall? (variable `uninterpolated`)\n",
    "4. What is the interpolated precision at 33% recall? (variable `interpolated`)\n",
    "5. Assume that these 20 documents are the complete result set of the system. What is the AP for the query?<br/><br/>\n",
    "Assume, now, instead, that the system returned the entire 10,000 documents in a ranked list, and these are the first 20 results returned.<br/><br/>\n",
    "6. What is the largest possible AP that this system could have? (variable `largest_AP`)\n",
    "7. What is the smallest possible AP that this system could have? (variable `smallest_AP`)\n",
    "8. In a set of experiments, only the top 20 results are evaluated by hand. The result in (e) is used to approximate the range (f)–(g). For this example, how large (in absolute terms) can the error for the AP be by calculating (e) instead of (f) and (g) for this query? (variable `Error`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "15b06a1b04586c07b10c9cda022f8c03",
     "grade": false,
     "grade_id": "cell-6ececd8d73bfc0ca",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.3,\n",
       " 0.4285714285714285,\n",
       " 1.2,\n",
       " 0.9090909090909091,\n",
       " 3.3303030303030305,\n",
       " 8.0,\n",
       " 0.0008002801400784469,\n",
       " 2)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "relevant = 6\n",
    "tot_relevant = 8\n",
    "tot = 20\n",
    "recall = relevant/tot_relevant\n",
    "\n",
    "precision = relevant/tot\n",
    "F1 = (2 * precision * recall) / (precision + recall)\n",
    "uninterpolated = precision / 0.25\n",
    "interpolated = precision / 0.33\n",
    "AP_query = 1/1 + 2/2 + 3/9 + 4/11 + 5/15 + 6/20\n",
    "largest_AP = 1/1 + 2/2 + 3/3 + 4/4 + 5/5 + 6/6 + 7/7 + 8/8\n",
    "smallest_AP = 1/9993 + 1/9994 + 1/9995 + 1/9996 + 1/9997 + 1/9998 + 1/9999 + 1/10000\n",
    "Error = 8 - 6 #?\n",
    "\n",
    "precision, F1, uninterpolated, interpolated, AP_query, largest_AP, smallest_AP, Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "692448c0d136e60c5ced6e3abe8f4fd3",
     "grade": true,
     "grade_id": "cell-c2bb032e00c4dee1",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6d4b52d8e67f79a97cf5bf0a5f0b59ed",
     "grade": true,
     "grade_id": "cell-464b3522aa196af7",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(F1) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "217d3651897af1c4b8fbee04ce310b18",
     "grade": true,
     "grade_id": "cell-ee2af88f21b18a7d",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(uninterpolated) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "daf0f299ac6ecc5f7545a662cbd9e16a",
     "grade": true,
     "grade_id": "cell-ee2af88f21b18a7dex",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(interpolated) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5d6fe375a8f2dec0a947b37f066af0b",
     "grade": true,
     "grade_id": "cell-dd18f398ad9a11ea",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(AP_query) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8d4a9d3e1f376f843680fb044f6f79b5",
     "grade": true,
     "grade_id": "cell-43dee696647831ac",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(largest_AP) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eeae02e405950f380a48bdecbdc7a06d",
     "grade": true,
     "grade_id": "cell-6efcd2c3d9ed173c",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(smallest_AP) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2d437ef9bd15ea427fc681b643afb534",
     "grade": true,
     "grade_id": "cell-dcd93ad480d010ac",
     "locked": true,
     "points": 0.1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(Error) in [float,int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3aef48cb18e14805cdb8213df5707eee",
     "grade": false,
     "grade_id": "cell-653e5c66190845a3",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.10\n",
    "Below is a table showing how two human judges rated the relevance of a set of 12 documents to a particular information need (0 = nonrelevant, 1 = relevant). Let us assume that you’ve written an IR system that for this query returns the set of documents {4, 5, 6, 7, 8}\n",
    "\n",
    "| docId | Judge 1 | Judge 2|\n",
    "|------:|:--------|:-------|\n",
    "| 1     | 0       | 0      |\n",
    "| 2     | 0       | 0      |\n",
    "| 3     | 1       | 1      |\n",
    "| 4     | 1       | 1      |\n",
    "| 5     | 1       | 0      |\n",
    "| 6     | 1       | 0      |\n",
    "| 7     | 1       | 0      |\n",
    "| 8     | 1       | 0      |\n",
    "| 9     | 0       | 1      |\n",
    "| 10    | 0       | 1      |\n",
    "| 11    | 0       | 1      |\n",
    "| 12    | 0       | 1      |\n",
    "\n",
    "1. Calculate the kappa measure between the two judges.\n",
    "2. Calculate precision, recall, and F1 of your system if a document is considered relevant only if the two judges agree.\n",
    "3. Calculate precision, recall, and F1 of your system if a document is considered relevant if either judge thinks it is relevant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| docId | Judge 1 | Judge 2|and|or|\n",
    "|------:|:--------|:-------|:--|--|\n",
    "| 1     | 0       | 0      |0  |0 |\n",
    "| 2     | 0       | 0      |0  |0 |\n",
    "| 3     | 1       | 1      |1  |1 |\n",
    "|------:|:--------|:-------|:--|--|\n",
    "| 4     | 1       | 1      |1  |1 |\n",
    "| 5     | 1       | 0      |0  |1 |\n",
    "| 6     | 1       | 0      |0  |1 |\n",
    "| 7     | 1       | 0      |0  |1 |\n",
    "| 8     | 1       | 0      |0  |1 |\n",
    "|------:|:--------|:-------|:--|--|\n",
    "| 9     | 0       | 1      |0  |1 |\n",
    "| 10    | 0       | 1      |0  |1 |\n",
    "| 11    | 0       | 1      |0  |1 |\n",
    "| 12    | 0       | 1      |0  |1 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eb4c1b27e6776637331d142902b8ccac",
     "grade": false,
     "grade_id": "cell-72efecf88879db26",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.0\n",
      "0.2 0.5 0.28571428571428575\n",
      "1.0 0.5 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "e = 0.5\n",
    "kappa = (2 - e) / (1 - e)\n",
    "\n",
    "# if the two judges agree.\n",
    "precision1 =  1/5\n",
    "recall1 =  1/2\n",
    "F1score1 = (2 * precision1 * recall1) / (precision1 + recall1)\n",
    "\n",
    "# if either judge thinks it is relevant.\n",
    "precision2 = 5/5\n",
    "recall2 = 5/10\n",
    "F1score2 = (2 * precision2 * recall2) / (precision2 + recall2)\n",
    "\n",
    "\n",
    "print(kappa)\n",
    "print(precision1, recall1, F1score1)\n",
    "print(precision2, recall2, F1score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ad86d4c9429ec0ee3fe6b13676b9c72c",
     "grade": true,
     "grade_id": "cell-d0b49ee6b738d816",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(kappa) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "627faf4e76f87e5cb92feb67972bb9fa",
     "grade": true,
     "grade_id": "cell-6c4430b58ad99642",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision1) in [float,int]\n",
    "assert type(recall1) in [float,int]\n",
    "assert type(F1score1) in [float,int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "df6af0266e84e0a9a1d839637e6bba72",
     "grade": true,
     "grade_id": "cell-b4ddeb8ad1122e7e",
     "locked": true,
     "points": 0.34,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert type(precision2) in [float,int]\n",
    "assert type(recall2) in [float,int]\n",
    "assert type(F1score2) in [float,int]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "278dc547a0840b2625d16df9ef63fd62",
     "grade": false,
     "grade_id": "cell-db608e43be498940",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "28898816090b94a56863e26251744777",
     "grade": false,
     "grade_id": "em-v",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Evaluation measures\n",
    "\n",
    "1. Define a function `Rprecision(ranked_list_of_results,list_of_relevant_objects)` which does what it says, it returns the R-precision given the input data.\n",
    "2. Define a function `AveragePrecision(ranked_list_of_results,list_of_relevant_objects)` which returns the average precision of this list of results given the list of relevant answers.\n",
    "\n",
    "Both functions of course come with one or two well chosen tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "de18476927f4ff49054c47b24b947ed4",
     "grade": false,
     "grade_id": "em-a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def Rprecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "    ranked_list_of_results = ranked_list_of_results[:len(list_of_relevant_objects)]\n",
    "    relevant_docs = 0\n",
    "    for docId in ranked_list_of_results:\n",
    "        if docId in list_of_relevant_objects:\n",
    "            relevant_docs += 1\n",
    "    return relevant_docs / len(list_of_relevant_objects)\n",
    "    \n",
    "    \n",
    "    \n",
    "def calcAP(relevance):\n",
    "    result = 0\n",
    "    relevant_count = 0\n",
    "    \n",
    "    for i,relevant in enumerate(relevance):\n",
    "        if relevant:\n",
    "            relevant_count += 1\n",
    "            result += relevant_count / (i + 1)\n",
    "            \n",
    "    return result\n",
    "\n",
    "def AveragePrecision(ranked_list_of_results, list_of_relevant_objects):\n",
    "    relevance = []\n",
    "    for docId in ranked_list_of_results:\n",
    "        relevant = docId in list_of_relevant_objects\n",
    "        relevance.append(relevant)\n",
    "\n",
    "    return calcAP(relevance)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(Rprecision([1, 2, 3, 4], [1, 2, 3, 4]), 1)\n",
    "assert_equal(Rprecision([1, 2, 3, 4], [1, 2]), 1)\n",
    "assert_equal(Rprecision([1, 2, 3, 4], [2, 3]), 0.5)\n",
    "assert_equal(Rprecision([1, 2, 3, 4], [3, 4]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert_equal(AveragePrecision([1, 2, 3, 4], [1, 2, 3, 4]), 4)\n",
    "assert_equal(AveragePrecision([1, 2, 3, 4], [1, 2]), 1/1 + 2/2)\n",
    "assert_equal(AveragePrecision([1, 2, 3, 4], [3, 4]), 1/3 + 2/4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9191f5774f4535d104d1fe42ae7b5db9",
     "grade": true,
     "grade_id": "em-t1",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Rprecision([1], [1]), float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4d05f65deb46ae4ba034726c7805266c",
     "grade": true,
     "grade_id": "em-t2",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(AveragePrecision([1], [1]), float)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bcac3e11641ce8c14ae19db88da51fc4",
     "grade": false,
     "grade_id": "pr",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Precision recall curve\n",
    "\n",
    "We create the interpolated precision-recall curve as in Figure 8.2 in MRS for one topic. We use for this the file \n",
    "`qrels.robust2004.txt`.\n",
    "\n",
    "It is easy to read in your data using pandas. We give some code to get you started.\n",
    "\n",
    "Store your answer in the dict `PR` using the provided schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46211e4c53a958b9fba1b3e925b1e6a6",
     "grade": false,
     "grade_id": "prc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AP: topic 301 147.90487207968027\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TOPIC</th>\n",
       "      <th>ITERATION</th>\n",
       "      <th>DOCUMENT_ID</th>\n",
       "      <th>RELEVANCY</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10082</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10169</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10243</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10319</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>301</td>\n",
       "      <td>0</td>\n",
       "      <td>FBIS3-10397</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TOPIC  ITERATION  DOCUMENT_ID  RELEVANCY\n",
       "0    301          0  FBIS3-10082          1\n",
       "1    301          0  FBIS3-10169          0\n",
       "2    301          0  FBIS3-10243          1\n",
       "3    301          0  FBIS3-10319          0\n",
       "4    301          0  FBIS3-10397          1"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qrels= pd.read_csv('qrels.robust2004.txt', sep=' ', header=None, \n",
    "                   names=['TOPIC',      'ITERATION',      'DOCUMENT_ID',     'RELEVANCY'])# https://trec.nist.gov/data/robust/qrels.robust2004.txt\n",
    "\n",
    "\n",
    "# Create Figure8\n",
    "\n",
    "rankedlist= qrels[(qrels.TOPIC==301)].DOCUMENT_ID.unique()\n",
    "np.random.shuffle(rankedlist) # This causes that the PR curve is different all the time (and often quite weird)\n",
    "relevantdocs=set(qrels[(qrels.TOPIC==301)&(qrels.RELEVANCY==1)].DOCUMENT_ID.unique() )\n",
    "\n",
    "N= len(relevantdocs)\n",
    "Rlevels=[(r+1) /N for r in range(len(relevantdocs))] \n",
    "\n",
    "print('AP: topic 301', AveragePrecision(rankedlist,relevantdocs  ))\n",
    "\n",
    "qrels.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "01027c61279685324d810cc33e4b7e55",
     "grade": false,
     "grade_id": "pra",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create Figure8.2 for topic 301\n",
    " \n",
    "# You must calculate the correct precison values for every given recall value here\n",
    "PR={\n",
    "    'UninterpolatedP':{\n",
    "        Rlevels[i]:AveragePrecision(rankedlist[:i], relevantdocs) / len(relevantdocs) for i in range(len(Rlevels))\n",
    "    },\n",
    "    'InterpolatedP':{\n",
    "        r:.6 for r in Rlevels\n",
    "    }\n",
    "}\n",
    "\n",
    "PR['UninterpolatedP'][Rlevels[0]] = 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "pd.DataFrame(PR).plot();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "326d67024d102de958f6dab1f96d765f",
     "grade": true,
     "grade_id": "prt",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert set(PR.keys())=={'UninterpolatedP', 'InterpolatedP'}\n",
    "for K in PR:\n",
    "     assert isinstance(PR[K],dict)\n",
    "assert_equal(set(PR['UninterpolatedP'].keys()),set(Rlevels))\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d409e7643408910bcf2f768c2df01899",
     "grade": false,
     "grade_id": "ck-v",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### Cohen's Kappa\n",
    "\n",
    "Compute the chance $P(E)$ used in the calculation of Cohen's Kappa, using the pooled marginals method.\n",
    "\n",
    "Your data looks like that given in Exercise 8.10: An array of pairs with binary relevance judgements in which the first column contains the scores of the first judge, etc.\n",
    "\n",
    "Hint: `numpy` arrays have all kind of handy methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4416d9b5b37301b3a183748753f90b7",
     "grade": false,
     "grade_id": "ck-a",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'P_E' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-105-a8f0b6777735>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mreturn\u001b[0m    \u001b[0mP_E\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-105-a8f0b6777735>\u001b[0m in \u001b[0;36mPE\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0;32mreturn\u001b[0m    \u001b[0mP_E\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mPE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'P_E' is not defined"
     ]
    }
   ],
   "source": [
    "# input data size of kappa must be like this\n",
    "data= np.array([[1,1],[1,0],[0,1],[1,1],[0,0]])\n",
    "\n",
    "def PE(data):\n",
    "    '''On input `data`, return the   P(E) (expected agreement).'''\n",
    "    data = np.array(data)\n",
    "    \n",
    "    return P_E \n",
    "\n",
    "PE(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8cf21d47e9b7070cf58716b8627b85aa",
     "grade": true,
     "grade_id": "ck-t",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(PE(data), float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "627ad98ff8f230481ad539fd36a2df43",
     "grade": false,
     "grade_id": "cell-891a3bbce2d771fa",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 10.1 Creating a test set\n",
    "\n",
    "Create a test collection in order to evaluate your two search engines (cosine similarity and BM25) on the set of Shakespeare. Create five information needs, which you express as queries. Use multi term queries only. For each information need you specify\n",
    "\n",
    " * The information need in natural language\n",
    " * The search query\n",
    " * A description when a document is considered relevant and when it is not. This should be a very clear description. A person assessing relevance may only use this together with the information need to determine relevance of a document. In particular the judge should not see the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52040542d5f6c5c6fea8d35ad4a1ffb5",
     "grade": true,
     "grade_id": "cell-3fd2c07428004c67",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "16f16b8c95fcffd6029b5cfde151fa75",
     "grade": false,
     "grade_id": "cell-1af8400eea1e3e8e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 10.2 Assess search engine results\n",
    "\n",
    "Assess for your two search engines, for all five information needs the first 10 results given back by the systems. Let another student do the same task as well.\n",
    "1. Using your own relevance judgments, compute for each system, the precision at 10 for each query, and the average P@10. Discuss the results.\n",
    "2. You have now two times at least 50 relevance judgements, and probably more. Compute the kappa measure between the two judges as in exercise 8.10, using pooled marginals as in Table 8.2. Discuss your results and try to explain them.\n",
    "3. Now compute P@10 for both systems again, once using the relevance judgments of the other assessor, once in which you say that a document is relevant if BOTH judges said it was relevant, and once in which you say that a document is relevant if AT LEAST ONE of the judges said it was relevant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "668cb299c87264fda9c0bb50078702d1",
     "grade": true,
     "grade_id": "cell-1f5104381710e6d4",
     "locked": false,
     "points": 3,
     "schema_version": 3,
     "solution": true
    }
   },
   "source": [
    "YOUR ANSWER HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "64b042a414ab4186ff36da8de10fc0ca",
     "grade": false,
     "grade_id": "cell-0828c47c6c841ab2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Programming (Inverted Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "45265903cced267bded49289c1a33748",
     "grade": false,
     "grade_id": "cell-ee70e59824b4530f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def loadShakespeare():\n",
    "    if 'shaks200.zip' in os.listdir():\n",
    "        return 'shaks200.zip'\n",
    "    elif os.path.exists('../../data/Week3/'):\n",
    "        return '../../data/Week3/shaks200.zip'\n",
    "    elif os.path.exists('../../../data/Week3/'):\n",
    "        return '../../../data/Week3/shaks200.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0a291330f776161fa4f004ba6e02d511",
     "grade": false,
     "grade_id": "cell-2cd22bbe0ed6e5b5",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 12 Positional inverted index\n",
    "Adjust your software making an inverted index for the Shakespeare collection: the output should be a positional index. A positional index is an index that also stores the position of the occurrences of a word in a document. The following figure shows an example of a positional index.\n",
    "\n",
    "<img src=\"http://nlp.stanford.edu/IR-book/html/htmledition/img121.png\" />\n",
    "\n",
    "Make the index of the form (defaultdicts are also allowed!):\n",
    "```\n",
    "{'caesar':   {'files': \n",
    "                 {'lll': {'list': [25350], 'total': 1},\n",
    "                  'm_for_m': {'list': [6773, 14734], 'total': 2},\n",
    "                  'm_wives': {'list': [3459], 'total': 1},\n",
    "                  'macbeth': {'list': [9256], 'total': 1},\n",
    "                  'othello': {'list': [11728], 'total': 1},\n",
    "                  'rich_ii': {'list': [22695], 'total': 1},\n",
    "                  'rich_iii': {'list': [16418, 16570, 30799, 30801], 'total': 4},\n",
    "                  'titus': {'list': [368], 'total': 1},\n",
    "                  ....\n",
    "                  },\n",
    "               'total': 604}....\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5d6c8aae378a5c4105975e2845550b86",
     "grade": true,
     "grade_id": "cell-498f786fcde3299d",
     "locked": false,
     "points": 2,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def positional_inverted_index(zipfolder):\n",
    "    # With zipfile we can read the file without opening the zip file\n",
    "    archive = zipfile.ZipFile(zipfolder, 'r')\n",
    "    namelist = [x for x in archive.namelist() if '.xml' in x]\n",
    "    \n",
    "    for infile in tqdm_notebook(namelist): # loop over each file\n",
    "        \n",
    "                \n",
    "    return positional_index\n",
    "positional_index = positional_inverted_index(loadShakespeare())\n",
    "positional_index['ass']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b6e21d8ffeb36779f0b33421a0797a57",
     "grade": true,
     "grade_id": "cell-29881806b7e5b184",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "positional_index = positional_inverted_index(loadShakespeare())\n",
    "assert type(positional_index) in [dict, defaultdict]\n",
    "assert_equal(set(positional_index['the'].keys()), {'total','files'})\n",
    "assert_equal(type(positional_index['the']['total']), int)\n",
    "assert_equal(set(positional_index['the']['files']['lll'].keys()), {'total','list'})\n",
    "assert_equal(type(positional_index['the']['files']['lll']['list']), list)\n",
    "assert_equal(type(positional_index['the']['files']['lll']['total']), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
