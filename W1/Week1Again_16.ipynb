{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4p7qAYAph4cJ"
   },
   "source": [
    "# Zoekmachines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O-7pNDymh4cL"
   },
   "source": [
    "## Notebook made by\n",
    "\n",
    "__Name__: Jesse Belleman, Levi van der Griendt, Naomi Rood, Daan van der Veen\n",
    "\n",
    "__Student id__ : 12632295, 11399589, 12666866, 12156469"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2LlgQTGeh4cL"
   },
   "source": [
    "## Toelichting\n",
    "\n",
    "* De meeste opgaven worden automatisch nagekeken. Bij vrijwel alle opdrachten staan er een paar zichtbare tests onder de opdracht, dit is voornamelijk om te zorgen dat je de juiste type output geeft. De andere tests worden na inleveren toegevoegd aan die cell.\n",
    "\n",
    "## Voor het inleveren!\n",
    "\n",
    "* Pas niet de cellen aan, vooral niet die je niet kunt editen. Dit levert problemen op bij nakijken. Twijfel je of je per ongeluk iets hebt gewijzigd, kopieer dan bij inleveren je antwoorden naar een nieuw bestand, zodat het niet fout kan gaan.\n",
    "\n",
    "* Zorg dat de code goed runt van boven naar beneden, verifieer dat door boven in Kernel -> Restart & Run All uit te voeren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXbbPYvjh4cM",
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Zoekmachines\" data-toc-modified-id=\"Zoekmachines-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;</span>Zoekmachines</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notebook-made-by\" data-toc-modified-id=\"Notebook-made-by-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;</span>Notebook made by</a></span></li><li><span><a href=\"#Toelichting\" data-toc-modified-id=\"Toelichting-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;</span>Toelichting</a></span></li><li><span><a href=\"#Voor-het-inleveren!\" data-toc-modified-id=\"Voor-het-inleveren!-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;</span>Voor het inleveren!</a></span></li></ul></li><li><span><a href=\"#Week-1\" data-toc-modified-id=\"Week-1-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;</span>Week 1</a></span></li><li><span><a href=\"#Book-Questions-(Chapter-1)\" data-toc-modified-id=\"Book-Questions-(Chapter-1)-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;</span>Book Questions (Chapter 1)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Exercise-1.1\" data-toc-modified-id=\"Exercise-1.1-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;</span>Exercise 1.1</a></span></li><li><span><a href=\"#Exercise-1.2a\" data-toc-modified-id=\"Exercise-1.2a-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;</span>Exercise 1.2a</a></span></li><li><span><a href=\"#Exercise-1.2b\" data-toc-modified-id=\"Exercise-1.2b-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;</span>Exercise 1.2b</a></span></li><li><span><a href=\"#Exercise-1.3\" data-toc-modified-id=\"Exercise-1.3-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;</span>Exercise 1.3</a></span></li><li><span><a href=\"#Exercise-1.4-(not-graded)\" data-toc-modified-id=\"Exercise-1.4-(not-graded)-3.5\"><span class=\"toc-item-num\">3.5&nbsp;&nbsp;</span>Exercise 1.4 (not graded)</a></span></li><li><span><a href=\"#Exercise-1.5-(not-graded)\" data-toc-modified-id=\"Exercise-1.5-(not-graded)-3.6\"><span class=\"toc-item-num\">3.6&nbsp;&nbsp;</span>Exercise 1.5 (not graded)</a></span></li><li><span><a href=\"#Exercise-1.6-(not-graded)\" data-toc-modified-id=\"Exercise-1.6-(not-graded)-3.7\"><span class=\"toc-item-num\">3.7&nbsp;&nbsp;</span>Exercise 1.6 (not graded)</a></span></li><li><span><a href=\"#Exercise-1.7-(not-graded)\" data-toc-modified-id=\"Exercise-1.7-(not-graded)-3.8\"><span class=\"toc-item-num\">3.8&nbsp;&nbsp;</span>Exercise 1.7 (not graded)</a></span></li><li><span><a href=\"#Exercise-1.8-(not-graded)\" data-toc-modified-id=\"Exercise-1.8-(not-graded)-3.9\"><span class=\"toc-item-num\">3.9&nbsp;&nbsp;</span>Exercise 1.8 (not graded)</a></span></li><li><span><a href=\"#Exercise-1.9-(not-graded)\" data-toc-modified-id=\"Exercise-1.9-(not-graded)-3.10\"><span class=\"toc-item-num\">3.10&nbsp;&nbsp;</span>Exercise 1.9 (not graded)</a></span></li></ul></li><li><span><a href=\"#Inverted-Index\" data-toc-modified-id=\"Inverted-Index-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;</span>Inverted Index</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;</span>Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-4.2\"><span class=\"toc-item-num\">4.2&nbsp;&nbsp;</span>Question 2</a></span></li><li><span><a href=\"#3\" data-toc-modified-id=\"3-4.3\"><span class=\"toc-item-num\">4.3&nbsp;&nbsp;</span>3</a></span></li><li><span><a href=\"#4\" data-toc-modified-id=\"4-4.4\"><span class=\"toc-item-num\">4.4&nbsp;&nbsp;</span>4</a></span></li><li><span><a href=\"#5\" data-toc-modified-id=\"5-4.5\"><span class=\"toc-item-num\">4.5&nbsp;&nbsp;</span>5</a></span></li><li><span><a href=\"#8-Boolean-Search\" data-toc-modified-id=\"8-Boolean-Search-4.6\"><span class=\"toc-item-num\">4.6&nbsp;&nbsp;</span>8 Boolean Search</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.1-Boolean-negation-search\" data-toc-modified-id=\"8.1-Boolean-negation-search-4.6.1\"><span class=\"toc-item-num\">4.6.1&nbsp;&nbsp;</span>8.1 Boolean negation search</a></span></li><li><span><a href=\"#8.2-Boolean-OR-search\" data-toc-modified-id=\"8.2-Boolean-OR-search-4.6.2\"><span class=\"toc-item-num\">4.6.2&nbsp;&nbsp;</span>8.2 Boolean OR search</a></span></li><li><span><a href=\"#8.3-Ranked-boolean-search\" data-toc-modified-id=\"8.3-Ranked-boolean-search-4.6.3\"><span class=\"toc-item-num\">4.6.3&nbsp;&nbsp;</span>8.3 Ranked boolean search</a></span><ul class=\"toc-item\"><li><span><a href=\"#8.3.1-Most-words-first\" data-toc-modified-id=\"8.3.1-Most-words-first-4.6.3.1\"><span class=\"toc-item-num\">4.6.3.1&nbsp;&nbsp;</span>8.3.1 Most-words first</a></span></li><li><span><a href=\"#8.3.2-Term-frequency-sum\" data-toc-modified-id=\"8.3.2-Term-frequency-sum-4.6.3.2\"><span class=\"toc-item-num\">4.6.3.2&nbsp;&nbsp;</span>8.3.2 Term-frequency sum</a></span></li><li><span><a href=\"#8.3.3-Combined\" data-toc-modified-id=\"8.3.3-Combined-4.6.3.3\"><span class=\"toc-item-num\">4.6.3.3&nbsp;&nbsp;</span>8.3.3 Combined</a></span></li></ul></li><li><span><a href=\"#8.4-Boolean-AND-search\" data-toc-modified-id=\"8.4-Boolean-AND-search-4.6.4\"><span class=\"toc-item-num\">4.6.4&nbsp;&nbsp;</span>8.4 Boolean AND search</a></span></li><li><span><a href=\"#8.5-Ranked-Boolean-AND-search-(not-graded)\" data-toc-modified-id=\"8.5-Ranked-Boolean-AND-search-(not-graded)-4.6.5\"><span class=\"toc-item-num\">4.6.5&nbsp;&nbsp;</span>8.5 Ranked Boolean AND search (not graded)</a></span></li><li><span><a href=\"#8.6-Relaxed-boolean-AND-search-(not-graded)\" data-toc-modified-id=\"8.6-Relaxed-boolean-AND-search-(not-graded)-4.6.6\"><span class=\"toc-item-num\">4.6.6&nbsp;&nbsp;</span>8.6 Relaxed boolean AND search (not graded)</a></span></li></ul></li></ul></li><li><span><a href=\"#SQL-and-full-text-search\" data-toc-modified-id=\"SQL-and-full-text-search-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;</span>SQL and full text search</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "mAMjie0Ch4cP",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7da289e22d804f86f67e22b517a1f5a7",
     "grade": false,
     "grade_id": "cell-22413888e92328cf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Week 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "mZKV2ANmh4cR",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e5aca405e4f5d718f0a19e6a6c567d7b",
     "grade": false,
     "grade_id": "cell-aff3c83a3bbf3bda",
     "locked": true,
     "schema_version": 3,
     "solution": false
    },
    "outputId": "9c787457-13ba-4164-f43b-9485010e7345"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nose\n",
      "  Downloading nose-1.3.7-py3-none-any.whl (154 kB)\n",
      "\u001b[?25l\r",
      "\u001b[K     |██▏                             | 10 kB 23.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████▎                           | 20 kB 32.4 MB/s eta 0:00:01\r",
      "\u001b[K     |██████▍                         | 30 kB 39.7 MB/s eta 0:00:01\r",
      "\u001b[K     |████████▌                       | 40 kB 28.2 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████▋                     | 51 kB 16.2 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████▊                   | 61 kB 17.6 MB/s eta 0:00:01\r",
      "\u001b[K     |██████████████▉                 | 71 kB 13.2 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████               | 81 kB 14.7 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████             | 92 kB 15.6 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████▏          | 102 kB 13.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████▎        | 112 kB 13.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████▍      | 122 kB 13.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████▌    | 133 kB 13.5 MB/s eta 0:00:01\r",
      "\u001b[K     |█████████████████████████████▋  | 143 kB 13.5 MB/s eta 0:00:01\r",
      "\u001b[K     |███████████████████████████████▊| 153 kB 13.5 MB/s eta 0:00:01\r",
      "\u001b[K     |████████████████████████████████| 154 kB 13.5 MB/s \n",
      "\u001b[?25hInstalling collected packages: nose\n",
      "Successfully installed nose-1.3.7\n"
     ]
    }
   ],
   "source": [
    "!pip install nose\n",
    "from nose.tools import assert_equal, assert_not_equal\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sWO7wv6gh4cR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b4db12a8a877ae51f90c20a72e20ce08",
     "grade": false,
     "grade_id": "cell-56e8018c9f1dbb9b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Book Questions (Chapter 1)\n",
    "In this section you will make the exercises from the book, which is freely available online as a PDF at [nlp.stanford.edu/IR-book/](http://nlp.stanford.edu/IR-book/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BnRIs1ABh4cR",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b1ffae4eeddcafb5f30b734e70c5b945",
     "grade": false,
     "grade_id": "cell-89ad97fbee0cf6a7",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.1\n",
    "\n",
    "Draw the inverted index that would be built for the following document collection. (See Figure1.3 for an example.) \n",
    "\n",
    "**Doc 1** new home sales top forecasts \n",
    "\n",
    "**Doc 2** home sales rise in july \n",
    "\n",
    "**Doc 3** increase in home sales in july \n",
    "\n",
    "**Doc 4** july new home sales rise\n",
    "\n",
    "Do it in the form of a dictionary `answer1`, with as key the word, and as value a **list** of postings\n",
    "\n",
    "So for example: \n",
    "\n",
    "```\n",
    "{\"forecasts\":[1,2,3,4],\"home\":[1,2,3,4],...}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "A0wTFzUoh4cS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fc2603260043c1c38b904c851522011d",
     "grade": false,
     "grade_id": "cell-3bac5e77cf2cbf78",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "e8fb6193-cae9-4082-9325-b311b6bbf9d9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'forecasts': [1],\n",
       " 'home': [1, 2, 3, 4],\n",
       " 'in': [2, 3],\n",
       " 'increase': [3],\n",
       " 'july': [2, 3, 4],\n",
       " 'new': [1, 4],\n",
       " 'rise': [2, 4],\n",
       " 'sales': [1, 2, 3, 4],\n",
       " 'top': [1]}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer1 = {\"new\":[1,4], \"home\":[1,2,3,4], \"sales\":[1,2,3,4], \"top\":[1], \"forecasts\":[1], \"rise\":[2,4], \"in\":[2,3], \"july\":[2,3,4], \"increase\":[3]}\n",
    "\n",
    "answer1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "V7gi4n9hh4cS",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "91f350d3302f813c2ace4937f38f9b24",
     "grade": true,
     "grade_id": "cell-07d452fd885d4634",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(answer1), dict)\n",
    "for k,v in answer1.items():\n",
    "    assert_equal(type(k), str)\n",
    "    assert_equal(type(v), list)\n",
    "    for p in v:\n",
    "        assert_equal(type(p), int)\n",
    "        assert p in range(1,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "9-1eRsUHh4cT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "cccb1e229f44988b05019192c4df3232",
     "grade": false,
     "grade_id": "2-2",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.2a\n",
    "Consider these documents: \n",
    "\n",
    "**Doc 1** breakthrough drug for schizophrenia \n",
    "\n",
    "**Doc 2** new schizophrenia drug \n",
    "\n",
    "**Doc 3** new approach for treatment of schizophrenia \n",
    "\n",
    "**Doc 4** new hopes for schizophrenia patients\n",
    "\n",
    "a. Draw the term-document incidence matrix for this document collection.\n",
    "\n",
    "Do this in pandas below. The document numbers become the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "deletable": false,
    "id": "k45WWGpLh4cT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95cb3d79f327c5e585f03fe4fc730d03",
     "grade": false,
     "grade_id": "cell-2f3b26c7cf356c32",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "2bf3b087-e1ef-414a-ebeb-1bf1b93d6a35"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>breaktrough</th>\n",
       "      <th>drug</th>\n",
       "      <th>for</th>\n",
       "      <th>schizophrenia</th>\n",
       "      <th>new</th>\n",
       "      <th>approach</th>\n",
       "      <th>treatment</th>\n",
       "      <th>of</th>\n",
       "      <th>hopes</th>\n",
       "      <th>patients</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>document</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          breaktrough  drug  for  schizophrenia  ...  treatment  of  hopes  patients\n",
       "document                                         ...                                \n",
       "1                   1     1    1              1  ...          0   0      0         0\n",
       "2                   0     1    0              1  ...          0   0      0         0\n",
       "3                   0     0    1              1  ...          1   1      0         0\n",
       "4                   0     0    1              1  ...          0   0      1         1\n",
       "\n",
       "[4 rows x 10 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = [[1,1,1,1,1,0,0,0,0,0,0],[2,0,1,0,1,1,0,0,0,0,0],[3,0,0,1,1,1,1,1,1,0,0],[4,0,0,1,1,1,0,0,0,1,1]]\n",
    "\n",
    "\n",
    "# Create the pandas DataFrame \n",
    "answer2a = pd.DataFrame(data, columns = ['document','breaktrough','drug','for','schizophrenia','new','approach','treatment','of','hopes','patients']).set_index('document')\n",
    "\n",
    "answer2a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "S02ZrLtch4cT",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "69c55ebf9328085fd623909691a6ca24",
     "grade": true,
     "grade_id": "cell-ef420142acbdf1a0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(answer2a),pd.DataFrame)\n",
    "assert_equal(set(answer2a.index), {1,2,3,4})\n",
    "assert_equal(answer2a.sum().sum(),18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qxxfkOVhh4cT",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "78d9fae611017e143a5ac0bceda86ff2",
     "grade": false,
     "grade_id": "cell-a804fbd968be82ea",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.2b\n",
    "Consider these documents: \n",
    "\n",
    "**Doc 1** breakthrough drug for schizophrenia \n",
    "\n",
    "**Doc 2** new schizophrenia drug \n",
    "\n",
    "**Doc 3** new approach for treatment of schizophrenia \n",
    "\n",
    "**Doc 4** new hopes for schizophrenia patients\n",
    "\n",
    "a. Draw the inverted index representation for this collection, as in Figure 1.3\n",
    "\n",
    "Do it in the form of a dictionary `answer2b`, with as key the word, and as value a **list** of postings (similar to excercise 1.1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "ifpNbTymh4cU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d47eb946447184f0e8764ecdb7761fe2",
     "grade": false,
     "grade_id": "cell-c9191628886d80a2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "7fe24c81-ffbb-40e6-c16e-5dd88a24772b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'approach': [3],\n",
       " 'breaktrough': [1],\n",
       " 'drug': [1, 2],\n",
       " 'for': [1, 3, 4],\n",
       " 'hopes': [4],\n",
       " 'new': [2, 3, 4],\n",
       " 'of': [3],\n",
       " 'patients': [4],\n",
       " 'schizophrenia': [1, 2, 3, 4],\n",
       " 'treatment': [3]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer2b = {\"breaktrough\":[1],\"drug\":[1,2],\"for\":[1,3,4],\"schizophrenia\":[1,2,3,4],\"new\":[2,3,4],\"approach\":[3],\"treatment\":[3],\"of\":[3],\"hopes\":[4],\"patients\":[4]}\n",
    "\n",
    "answer2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "1K4ssoqCh4cU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "00c444051e748725679c9350dc5ee09f",
     "grade": true,
     "grade_id": "cell-539e112dcb672dcf",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(answer2b), dict)\n",
    "for k,v in answer2b.items():\n",
    "    assert_equal(type(k), str)\n",
    "    assert_equal(type(v), list)\n",
    "    for p in v:\n",
    "        assert_equal(type(p), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ISgacwx8h4cU",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8ef7a6eb12bed0e7c9c1adb4d3b5c550",
     "grade": false,
     "grade_id": "cell-42998a26a58d9689",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.3\n",
    "For the document collection shown in Exercise 1.2, what are the returned results for these queries: \n",
    "\n",
    "a. schizophrenia AND drug \n",
    "\n",
    "b. for AND NOT(drug OR approach)\n",
    "\n",
    "Give the answer as set of Docs (`answer3a` and `answer3b` respectively), so for example `set([1])` or `set([2,3,4])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "id": "d1zWrX_oh4cU",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1941b876b6e26e48d69ebc0fabc33882",
     "grade": false,
     "grade_id": "cell-e1a790e2b1c4e97c",
     "locked": false,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "caed24da-cf18-4dcf-fd65-3558e224e68a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({1, 2}, {4})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer3a = set([1,2])\n",
    "answer3b = set([4])\n",
    "\n",
    "answer3a, answer3b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2KgGXQTMh4cV",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "71728fd95ad4e154a9f15e26c05039d9",
     "grade": true,
     "grade_id": "cell-2c885e8cc5ef265a",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(answer3a), set)\n",
    "assert_equal(type(answer3b), set)\n",
    "for a in answer3a|answer3b:\n",
    "    assert_equal(type(a),int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gIPfo2WOh4cV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c7e0bb1b2e4c54e7871f30bcc10e8675",
     "grade": false,
     "grade_id": "cell-e87cd3bfd87cc745",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.4 (not graded)\n",
    "For the queries below, can we still run through the intersection in time O(x + y), where x and y are the lengths of the postings lists for Brutus and Caesar? If not, what can we achieve?\n",
    "\n",
    "a. Brutus AND NOT Caesar \n",
    "\n",
    "b. Brutus OR NOT Caesar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTQjHPe3h4cV"
   },
   "source": [
    "a. Yes the time is still in O(x+y).You just check the documents that occur in the first posting list and then make sure that these documents are not in the second posting list. So you just check two posting lists.\n",
    "\n",
    "b. Time is O(N), with N being the total amount of documents. This is because it tells us \"OR NOT\" ceasar. This means that there can possibly be no ceasar in all documents so in that case you would need to write down all document numbers instead of just the posting list behind ceasar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "6zttq9JEh4cV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "4b5fb4f863e8bed1f93944caf326776a",
     "grade": false,
     "grade_id": "cell-caf4ee8c0096ec64",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.5 (not graded)\n",
    "Extend the postings merge algorithm to arbitrary Boolean query formulas. What is its time complexity? For instance, consider:\n",
    "\n",
    "c. (Brutus OR Caesar) AND NOT (Antony OR Cleopatra)\n",
    "\n",
    "Can we always merge in linear time? Linear in what? Can we do better than this?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBdtQrC-h4cV"
   },
   "source": [
    "ANSWER: \n",
    "We merge in O(a N). a are the amount of terms we use in the query and N is the amount of documents. Therefore the intersection time is linear in terms of N and a. So in terms of the amount of document and the amount of terms used in the query. You can not do better than this because N is the amount of documents and these can all be in the result (for example if Antony OR Cleopatra are in no documents, the right side of the query becomes as big as N)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "qlP-5yewh4cV",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f2b820ad18985b8e9fc75857e0730dae",
     "grade": false,
     "grade_id": "cell-0e2eec0ba9ac7602",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.6 (not graded)\n",
    "We can use distributive laws for AND and OR to rewrite queries.\n",
    "\n",
    "a. Show how to rewrite the query in Exercise1.5 into disjunctive normal form using the distributive laws. \n",
    "\n",
    "b. Would the resulting query be more or less efﬁciently evaluated than the original formof this query? \n",
    "\n",
    "c. Is this result true in general or does it depend on the words and the contents of the document collection?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y3JqsYcbh4cW"
   },
   "source": [
    "ANSWER:\n",
    "a.\n",
    "(Brutus OR Caesar) AND NOT (Antony OR Cleopatra) = \n",
    "\n",
    "\n",
    "(Brutus AND NOT (Antony OR Cleopatra)  OR (Caesar AND NOT (Antony OR Cleopatra) = \n",
    "\n",
    "(Brutus AND NOT Antony AND NOT Cleopatra ) OR (Caesar AND NOT Antony AND NOT Cleopatra)\n",
    "\n",
    "\n",
    "b. This query is more efficient that the query in exc 1,5. This is because in exc 1,5 we start with two unions. These unions can become pretty big lists and then checking both these lists for intersections can take a long time. Whereas the newly written query starts with intersections on the smaller posting lists (instead of on the bigger created unions, like in the query of exc 1,5) to then only form a union of these lists. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "W6kpF_inh4cW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0ebc0d6269f37e215eae9ae6d96c9052",
     "grade": false,
     "grade_id": "cell-d94a04dec2bb608e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.7 (not graded)\n",
    "Recommend a query processing order for<br/>\n",
    "d. (tangerine OR trees) AND (marmalade OR skies) AND (kaleidoscope OR eyes) given the following postings list sizes:<br/>\n",
    "\n",
    "|Term |Postings size|\n",
    "|-----|:------------|\n",
    "|eyes|213312|\n",
    "|kaleidoscope|87009|\n",
    "|marmalade|107913|\n",
    "|skies|271658|\n",
    "|tangerine|46653|\n",
    "|trees|316812|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I4YPmbNph4cW"
   },
   "source": [
    "ANSWER: 1 = tangerine OR trees, which costs O(46653+316812) = O(363465)\n",
    "\n",
    "2 = marmalade OR skies, which costs O(107913+271658) = O(379571)\n",
    "\n",
    "3 = kaleidoscoop OR eyes, which costs O(87009 + 213312 = O(300321)\n",
    "\n",
    "It is therefore better to first merge the lists: (tangerine OR trees) AND (kaleidoscoop OR eyes) and then merge this result with the list (marmalade OR skies). This is because the shorter the lists during the AND operation the less time it takes and the most likely that the result of the intersection will be smaller. A smaller result will result in less time needed to do an intersection with the resulting list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "S53Qlxkch4cW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c0c7a356ee6c65df600a66886dee041a",
     "grade": false,
     "grade_id": "cell-55197cce39fe9337",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.8 (not graded)\n",
    "If the query is:<br/>\n",
    "\n",
    "e. friends AND romans AND (NOT countrymen)<br/>\n",
    "\n",
    "how could we use the frequency of countrymen in evaluating the best query evaluation order? In particular, propose a way of handling negation in determining the order of query processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NIzju1hph4cW"
   },
   "source": [
    "ANSWER: We know the amount of documents N. And we can check the size of the posting list of countrymen (s) and then take N-s to determine the query evaluation order."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "64ogikWth4cW",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "81753874a9e83cad360950446ecb4cac",
     "grade": false,
     "grade_id": "cell-e179614a88821ccc",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Exercise 1.9 (not graded)\n",
    "For a conjunctive query, is processing postings lists in order of size guaranteed to be optimal? Explain why it is, or give an example where it isn’t."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Szxe_8q-h4cX"
   },
   "source": [
    "ANSWER: It is not always guaranteed to be optimal. This is because you can not know the size of the intersections on forehand. Some big lists of 1000 words might have only a dozen words in common whilst one of these 1000 words list might have 100 words in common with a smaller list of 500 words. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "wUaAO1Y-h4cX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2e226824ba5b967f1ab6f6babe8fa198",
     "grade": false,
     "grade_id": "cell-b97c0ee75cddcf33",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# Inverted Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yC1rC-KWh4cX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "46fcede6764aae769bd818dd841ddf14",
     "grade": true,
     "grade_id": "cell-480ae65e69b5d934",
     "locked": true,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from tqdm import tqdm_notebook\n",
    "import glob\n",
    "import nltk\n",
    "import zipfile\n",
    "import math\n",
    "import pandas as pd\n",
    "import sys\n",
    "import itertools\n",
    "\n",
    "\n",
    "def loadShakespeare():\n",
    "    if 'shaks200.zip' in os.listdir():\n",
    "        return 'shaks200.zip'\n",
    "    elif os.path.exists('../../data/Week1/'):\n",
    "        return '../../data/Week1/shaks200.zip'\n",
    "    elif os.path.exists('../../../data/Week1/'):\n",
    "        return '../../../data/Week1/shaks200.zip'\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "pjQ2EtO7h4cX",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1eea4e854b768370d0808fc89ada1004",
     "grade": false,
     "grade_id": "cell-4abbd8297102b812",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 1\n",
    "Make a function `index_collection` which turns a zipfolder (like Shakespeare) into an inverted index.\n",
    "\n",
    "So the index will be of form:\n",
    "```\n",
    "{'the': defaultdict(int,\n",
    "                         {'a_and_c': 875,\n",
    "                          'all_well': 736,\n",
    "                          'as_you': 698,\n",
    "                          'com_err': 445,\n",
    "                          'coriolan': 1130,\n",
    "                          'cymbelin': 973,\n",
    "                          'dream': 565,\n",
    "                          'hamlet': 1146,\n",
    "                          ...\n",
    "```\n",
    "\n",
    "* Think about lower case, tokenization etc.\n",
    "* Doesn't have to be a defaultdict, but can be useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "deletable": false,
    "id": "Q55uPtY6h4cX",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e7ee2db8fdd5d973de41ae6d083d73fd",
     "grade": false,
     "grade_id": "cell-ecc0e099ac33f896",
     "locked": false,
     "points": 1,
     "schema_version": 3,
     "solution": true
    },
    "outputId": "6e9b09b9-f68a-45f8-a033-9dfbace160d5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-cb5222b61dfd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mMyIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time Shakespeare = index_collection(loadShakespeare())'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mmagic\u001b[0;34m(self, arg_s)\u001b[0m\n\u001b[1;32m   2158\u001b[0m         \u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpartition\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m         \u001b[0mmagic_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmagic_name\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefilter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mESC_MAGIC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2160\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m     \u001b[0;31m#-------------------------------------------------------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_line_magic\u001b[0;34m(self, magic_name, line)\u001b[0m\n\u001b[1;32m   2079\u001b[0m                 \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'local_ns'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_locals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2080\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2081\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2082\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<decorator-gen-53>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-cb5222b61dfd>\u001b[0m in \u001b[0;36mindex_collection\u001b[0;34m(zipfolder)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mindex_collection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipfolder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# With zipfile we can read the file without opening the zip file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0marchive\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipfolder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mnamelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamelist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'.xml'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mMyIndex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# initialize MyIndex\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel)\u001b[0m\n\u001b[1;32m   1256\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1257\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1258\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1259\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1260\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1319\u001b[0m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1321\u001b[0;31m             \u001b[0mendrec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_EndRecData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1322\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.7/zipfile.py\u001b[0m in \u001b[0;36m_EndRecData\u001b[0;34m(fpin)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;31m# Determine file size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m     \u001b[0mfpin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseek\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m     \u001b[0mfilesize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfpin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'seek'"
     ]
    }
   ],
   "source": [
    "from tqdm import notebook\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "\n",
    "def index_collection(zipfolder):\n",
    "    # With zipfile we can read the file without opening the zip file\n",
    "    archive = zipfile.ZipFile(zipfolder, 'r')\n",
    "    namelist = [x for x in archive.namelist() if '.xml' in x]\n",
    "    MyIndex = defaultdict(lambda: defaultdict(int)) # initialize MyIndex\n",
    "    for infile in notebook.tqdm(namelist): # loop over each file\n",
    "        f = archive.open(infile)\n",
    "        file_name = infile.replace('.xml', '')\n",
    "        for line in f.readlines():\n",
    "            # decode line from byte to string object\n",
    "            line = line.decode(\"utf-8\")\n",
    "            # search only for tet between two xml tags\n",
    "            line = re.search('<.+>(.+?)</.+>', line)\n",
    "            # if there was any text between the tags\n",
    "            if line:\n",
    "                # select between the tags and lowercase it\n",
    "                line = line.group(1)\n",
    "                line = line.lower()\n",
    "                # forms a list of the spoken words\n",
    "                line = word_tokenize(line)\n",
    "                \n",
    "                # iterate over the words in the line\n",
    "                for word in line:\n",
    "                    # check if word is a word not a punctuation mark\n",
    "                    if word[0].isalnum():\n",
    "\n",
    "                      # check if the word has already been mentioned in this play\n",
    "                      if file_name in MyIndex[word]:\n",
    "                          # then increment the count for this word in this play with 1\n",
    "                          MyIndex[word][file_name] += 1   \n",
    "                      # else add the mentioning of this word in this play to the dict\n",
    "                      else:\n",
    "                          MyIndex[word][file_name] = 1 \n",
    "                        \n",
    "    return MyIndex\n",
    "\n",
    "%time Shakespeare = index_collection(loadShakespeare())\n",
    "\n",
    "\n",
    "Shakespeare['the'], Shakespeare['witch']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "AQwL0T1Th4cY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7fffcffd9bf5e4ddb7c18bd3ed50ec2c",
     "grade": true,
     "grade_id": "cell-80a890412656a8d0",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert isinstance(Shakespeare, dict)\n",
    "assert 'macbeth' in Shakespeare['hurlyburly']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WMMufGFHh4cY",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7a69e7420ead48fa34a0e10ab20103d",
     "grade": false,
     "grade_id": "cell-26502d34bfd79644",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## Question 2\n",
    "\n",
    "Count the total number of tokens in the works of Shakespeare (`TotNumberofTokens`) and the total number of unique tokens (`Vocsize`).  Answer with two one-liners using your created index (which is called `Shakespeare`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "8WC019zRh4cY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "e8d1050d828629085a1f17a960a7b3ac",
     "grade": false,
     "grade_id": "cell-0d0ac196213493f2",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# hint: look up how you compute the sum of a list of numbers without a for loop or an extra variable\n",
    "Vocsize = len(Shakespeare.keys())\n",
    "TotNumberofTokens = sum([tot for words in  Shakespeare.values() for tot in words.values()])\n",
    "\n",
    "Vocsize, TotNumberofTokens, round(TotNumberofTokens/Vocsize,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "DcOGrHLyh4cY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "f7ced0fa61d4b133bea7a6962ae5c6c9",
     "grade": true,
     "grade_id": "cell-280ef3963b3a3ff0",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(Vocsize), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ZuvnvuLYh4cY",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "984c022402e571a58f8f9e6b2b46bfcc",
     "grade": true,
     "grade_id": "cell-543ceebff67e59c4",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(TotNumberofTokens), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2G34E5oMh4cZ",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "8f17cb4c1f9c60d76e16075e3cc110dd",
     "grade": false,
     "grade_id": "cell-c82f6c59b7826d9e",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3\n",
    "\n",
    "1. Calculate the document frequency (in how many documents occurs the term) for each term (store this in the dictionary named `DocFreq`) and \n",
    "2. the corpus frequency (how often does the term occur in the corpus) for each term (store this in the dictionary named `CorpusFreq`). \n",
    "3. Add these two values as new values in your inverted index, which will be called `NewIndex`.\n",
    "\n",
    "The `NewIndex` will then look like this:\n",
    "```{'the': {'CorpusFreq': 28254,\n",
    "  'DocFreq': 37,\n",
    "  'Freq_per_Doc': Counter({'a_and_c': 874,\n",
    "           'all_well': 736,\n",
    "           'as_you': 698,\n",
    "           'com_err': 444,\n",
    "           'coriolan': 1130,\n",
    "           'cymbelin': 973,\n",
    "           'dream': 565,\n",
    "           'hamlet': 1146,\n",
    "           'hen_iv_1': 869,\n",
    "           'hen_iv_2': 1001,\n",
    "           'hen_v': 1089,\n",
    "           'hen_vi_1': 731,\n",
    "           'hen_vi_2': 953,\n",
    "           'hen_vi_3': 818,\n",
    "           'hen_viii': 947,\n",
    "           'j_caesar': 607,\n",
    "           'john': 734,\n",
    "           'lear': 911,\n",
    "           'lll': 860,\n",
    "           'm_for_m': 702,\n",
    "           'm_wives': 610,\n",
    "           'macbeth': 736,\n",
    "           'merchant': 840,\n",
    "           'much_ado': 581,\n",
    "           'othello': 765,\n",
    "           'pericles': 635,\n",
    "           'r_and_j': 687,\n",
    "           'rich_ii': 752,\n",
    "           'rich_iii': 995,\n",
    "           't_night': 550,\n",
    "           'taming': 510,\n",
    "           'tempest': 521,\n",
    "           'timon': 510,\n",
    "           'titus': 659,\n",
    "           'troilus': 846,\n",
    "           'two_gent': 409,\n",
    "           'win_tale': 860})},...```\n",
    "           \n",
    " * the type of `Freq_per_Doc` doesn't have to be a Counter, can also be a defaultdict or dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "_pW9vmRhh4cZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a3c5192d35a59dd8a1e2f0a51b4e7eea",
     "grade": false,
     "grade_id": "cell-739b18d157f01e44",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "CorpusFreq = {word:sum(Shakespeare[word].values()) for word in Shakespeare.keys()} # change to your solution\n",
    "DocFreq = {word:len(Shakespeare[word].keys()) for word in Shakespeare.keys()} # change to your solution\n",
    "NewIndex = { word: {'CorpusFreq': CorpusFreq[word], \"DocFreq\": DocFreq[word],\n",
    "            \"Freq_per_Doc\": {doc: Shakespeare[word][doc] for doc in Shakespeare[word].keys()}} \n",
    "            for word in Shakespeare.keys() } # change to your solution\n",
    " \n",
    "\n",
    "\n",
    "NewIndex['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "yYnKrTK-h4cZ",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "0910d916e77369897679f66a135936d8",
     "grade": true,
     "grade_id": "cell-4cdc200e7c434c2b",
     "locked": true,
     "points": 0.34,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(CorpusFreq),dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "3KmKH1PUh4ca",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1a2425a1d73ecb52267dd536fe58a50a",
     "grade": true,
     "grade_id": "cell-9807e00ccea6371d",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(DocFreq),dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "azurH_Tmh4ca",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "95c84d6d0abc797579c8d53885ccbe65",
     "grade": true,
     "grade_id": "cell-dcf52b08a1da000e",
     "locked": true,
     "points": 0.33,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(NewIndex),dict)\n",
    "assert_equal(set(NewIndex['the'].keys()),{'Freq_per_Doc','DocFreq','CorpusFreq'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "ug2GL9Y0h4ca",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "72463d45525aa50078bfb485977f9692",
     "grade": false,
     "grade_id": "cell-34d5346f11a4792f",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 4\n",
    "\n",
    "Create a function `IndexInfo(index)` which from the enhanced index retrieves the following information:\n",
    "\n",
    "* size of the vocabulary\n",
    "* nr of terms with corpus frequency 1\n",
    "* nr of terms with document frequency 1 \n",
    "* nr of terms with document frequency  equal to the number of documents in the corpus\n",
    "* nr of terms with document frequency  half or more than the number of documents in the corpus\n",
    "\n",
    "It should return the information in that order!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "8i0r278-h4ca",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "007cf49d342dd5fe54f3f86f7e7fd266",
     "grade": false,
     "grade_id": "cell-30fe60f6fba5acab",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def documents_to_list(doclist,index):\n",
    "    for doc in index.keys():\n",
    "      if doc not in doclist:\n",
    "        doclist.append(doc);\n",
    "    return doclist\n",
    "\n",
    "def IndexInfo(index):\n",
    "    documents = []\n",
    "    half = {}\n",
    "    full = {}\n",
    "    Vocsize = 0\n",
    "    Hapaxes = 0\n",
    "    SingleDocTerms = 0\n",
    "    FullDocTerms = 0\n",
    "    HalfDocTerms = 0\n",
    "\n",
    "    # we going walk though the words\n",
    "    for word in index.keys():\n",
    "      Vocsize = Vocsize + 1\n",
    "      # check if it has a corpus frequency of 1 also have a document frequency of 1\n",
    "      if index[word]['CorpusFreq'] == 1:\n",
    "        Hapaxes = Hapaxes + 1\n",
    "        SingleDocTerms = SingleDocTerms + 1\n",
    "      # check if the is only 1 document.\n",
    "      elif index[word]['DocFreq'] == 1:\n",
    "        SingleDocTerms = SingleDocTerms + 1\n",
    "      # add all document this documents in is.\n",
    "      documents = documents_to_list(documents,index[word]['Freq_per_Doc'])\n",
    "      # add possible \n",
    "      if index[word][\"DocFreq\"] == len(documents):\n",
    "        full[word] = index[word][\"DocFreq\"]\n",
    "        half[word] = index[word][\"DocFreq\"]\n",
    "      elif index[word][\"DocFreq\"] == len(documents)/2:\n",
    "        half[word] = index[word][\"DocFreq\"]\n",
    "    \n",
    "    #now check if they all possible half and full length are acctual full and half.\n",
    "    for word  in full.keys():\n",
    "      if len(documents) == full[word]:\n",
    "        FullDocTerms = FullDocTerms + 1\n",
    "    #here we do the same as above only for half or more\n",
    "    halfsize = len(documents)/2\n",
    "    for word in half.keys():\n",
    "      if halfsize <= half[word]:\n",
    "        HalfDocTerms = HalfDocTerms + 1\n",
    "      \n",
    "      \n",
    "    \n",
    "\n",
    "    return Vocsize, Hapaxes, SingleDocTerms, FullDocTerms, HalfDocTerms\n",
    "IndexInfo(NewIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GtnRtudYh4ca",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "511dbdee9c1cafbf2684b07a324fa13f",
     "grade": true,
     "grade_id": "cell-ca98643b040c6eac",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(IndexInfo(NewIndex)[0]), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "Kn9n8YtSh4ca",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cfc7d00c2d0d0118cfb7e21467bc6d21",
     "grade": true,
     "grade_id": "cell-322261b39549eb9b",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(IndexInfo(NewIndex)[1]), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "w-k_Abvoh4cb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "93f6eeb876c14c888f16e7d7990ddc42",
     "grade": true,
     "grade_id": "cell-301745b87a6b9806",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(IndexInfo(NewIndex)[2]), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "uS_Wbnb-h4cb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ecff151f6957d14a2db52cb7edb4e7e6",
     "grade": true,
     "grade_id": "cell-03e46e9370e4d44d",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(IndexInfo(NewIndex)[3]), int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "gd_EVBCrh4cb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "006ac3f56f59b8dfa9865a59bb55ee64",
     "grade": true,
     "grade_id": "cell-f5e4c181b23654c6",
     "locked": true,
     "points": 0.2,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(IndexInfo(NewIndex)[4]), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "naL9h6ITh4cb",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "47226f3dd1f28744f745fa62b808bf00",
     "grade": false,
     "grade_id": "cell-f313a1814da65d12",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 5\n",
    "\n",
    "Look up what Zipf's law means in Wikipedia or in the book. Now show that it holds or not for our corpus.\n",
    "\n",
    "Make the pandas dataframe  `CorpusFreqDF` so that `CorpusFreqDF.plot(loglog=True);` will return a Zipf plot of terms and frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "TALBXZnBh4cb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "5c6eeefdaf86fafe27e0b341ae28c4f0",
     "grade": false,
     "grade_id": "cell-b201e9f17c20805f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "CorpusFreqDF = pd.DataFrame([NewIndex[word]['CorpusFreq'] for word in NewIndex], columns=['CorpusFrequency'])\n",
    "CorpusFreqDF = CorpusFreqDF.sort_values(by=['CorpusFrequency'], ascending=False)\n",
    "CorpusFreqDF = CorpusFreqDF['CorpusFrequency'].reset_index()\n",
    "CorpusFreqDF = CorpusFreqDF['CorpusFrequency'].to_frame()\n",
    "\n",
    "CorpusFreqDF.plot(loglog=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "nYhIKyZyh4cb",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "4a85dc1f40ca597bff3209a3f5c5b05e",
     "grade": true,
     "grade_id": "cell-bb71005d7439ea25",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(CorpusFreqDF), pd.DataFrame)\n",
    "CorpusFreqDF.shape,CorpusFreqDF.CorpusFrequency.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "iup9qrFlh4cc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f752ad5b2c6ef2a8ea3c56b1f29983cb",
     "grade": false,
     "grade_id": "cell-d5c398e29d5bb097",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 8 Boolean Search\n",
    "\n",
    "We will now make a Boolean search engine as described in Chapter 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "GFAlA3sSh4cc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0e6042bc9e9d0ab6a89fd2c3ba0da71f",
     "grade": false,
     "grade_id": "cell-4e743addfb859948",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.1 Boolean negation search\n",
    "Create the function `booleanSearchNOT(word, index)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "DmHgJEpHh4cc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1487fd6a9b335fbc2303eaf4da904782",
     "grade": false,
     "grade_id": "cell-ee3349180eacd8da",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def booleanSearchNOT(word, index):\n",
    "    \"\"\"\n",
    "    This function takes a string as input and returns all document ids as set\n",
    "    which do NOT contain the word.\n",
    "    \"\"\"\n",
    "   \n",
    "    # set of all documents\n",
    "    doc_list = set()\n",
    "    for key in index.keys():\n",
    "        for doc in index[key]['Freq_per_Doc']:\n",
    "            doc_list.add(doc)    \n",
    "    \n",
    "    # set of all the documents that the word DOES occur in.\n",
    "    occurs = set(index[word]['Freq_per_Doc'].keys())\n",
    "    \n",
    "    return doc_list - occurs\n",
    "    \n",
    "booleanSearchNOT(\"lightly\", NewIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "molx92QMh4cc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bda287c47fa0e7a3e77d1d569a6151c2",
     "grade": true,
     "grade_id": "cell-f6125c2504b1a68f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(booleanSearchNOT(\"lightly\", NewIndex)), set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "WNrWJYhSh4cc",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "d8ebe23fc1518f950cf2c440735ebaf8",
     "grade": false,
     "grade_id": "cell-92d5b138626bc512",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.2 Boolean OR search\n",
    "\n",
    "Create the function `booleanSearchOR(query, index)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Uo_cKH1lh4cc",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bdac97efa6730b6935a664d4a3e62272",
     "grade": false,
     "grade_id": "cell-10aa5d1597de0be5",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def booleanSearchOR(query, index):\n",
    "    \"\"\"\n",
    "    This function takes a query as input and returns all document ids as set which\n",
    "    contain at least one of the words.\n",
    "    \"\"\"\n",
    "    result = set()\n",
    "    for word in query:\n",
    "        for document in index[word]['Freq_per_Doc']:\n",
    "            result.add(document)\n",
    "            \n",
    "    return result\n",
    "    \n",
    "    \n",
    "booleanSearchOR([\"lightly\", \"love\"], NewIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fBt5yOCxh4cd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "14a55161ea093543326880b304e34655",
     "grade": true,
     "grade_id": "cell-f8efc9467f05480f",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(booleanSearchOR([\"lightly\", \"love\"], NewIndex)), set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "BhsSuyKnh4cd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "7ea5a649276c2f6b07428e1003dabb01",
     "grade": false,
     "grade_id": "cell-191d11c78fbc1377",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.3 Ranked boolean search\n",
    "A Boolean search engine is not supposed to rank the documents it returns, but we can do that anyway.\n",
    "\n",
    "#### 8.3.1 Most-words first\n",
    "Rank the documents returned for the OR-query in last question as follows: documents which match most words are returned first (if equal, sort alphabetically on document ID). Return the document ID's followed by the number of matching words (i.e. the \"score\" of the document for this query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "YPQOhMP5h4cd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7265ef522ce89db3862a90f0bc528993",
     "grade": false,
     "grade_id": "cell-facf62845225258f",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def booleanSearchMatch(query, index):\n",
    "    \"\"\"\n",
    "    This function takes a query as input and returns a list of  all document ids and\n",
    "    scores of documents which contain at least one of the words. Documents\n",
    "    are ranked such that documents that match the most words are returned\n",
    "    first.\n",
    "    \"\"\"\n",
    "    \n",
    "    ranked = {}\n",
    "    \n",
    "    # add the documents with their count to a dict\n",
    "    for word in query:\n",
    "        for document in index[word]['Freq_per_Doc']:\n",
    "            if document in ranked.keys():\n",
    "                ranked[document] += 1\n",
    "            else:\n",
    "                ranked[document] = 1\n",
    "    \n",
    "    # sort the document on the count\n",
    "    ranked = sorted(ranked.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked\n",
    "    \n",
    "booleanSearchMatch([\"love\",\"lightly\"], NewIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "H00eiQRwh4cd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28f10dffd84a2c0955e14d0206892a77",
     "grade": true,
     "grade_id": "cell-0f1a4c699d049875",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(booleanSearchMatch([\"love\",\"lightly\"], NewIndex)), list)\n",
    "assert_equal(type(booleanSearchMatch([\"love\",\"lightly\"], NewIndex)[0]), tuple)\n",
    "assert_equal(type(booleanSearchMatch([\"love\",\"lightly\"], NewIndex)[0][0]), str)\n",
    "assert_equal(type(booleanSearchMatch([\"love\",\"lightly\"], NewIndex)[0][1]), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "jR7j3YcEh4cd",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "af347c31077e19a283ffb51380f83820",
     "grade": false,
     "grade_id": "cell-f37f87d6130a23c6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 8.3.2 Term-frequency sum\n",
    "Now rank the documents by the sum of the term frequencies for all query words. Return doc ID's and scores.\n",
    "\n",
    "Again first sort by score, and break ties by sorting  alphabetically on Doc ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "5LAhpDFvh4cd",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8c178edf235ab8a96410207be6c208f8",
     "grade": false,
     "grade_id": "cell-74c7e3f7e51b6c92",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def booleanSearchSumTF(query, index):\n",
    "    \"\"\"\n",
    "    This function takes a query as input and returns all document ids and\n",
    "    scores of documents which contain at least one of the words. Documents\n",
    "    are ranked by the sum of the term frequencies of all query words.\n",
    "    \"\"\"\n",
    "    \n",
    "    ranked = {}\n",
    "    \n",
    "    # add the documents with their count to a dict\n",
    "    for word in query:\n",
    "        for document in index[word]['Freq_per_Doc']:\n",
    "            if document in ranked.keys():\n",
    "                ranked[document] += index[word]['DocFreq']\n",
    "            else:\n",
    "                ranked[document] = index[word]['DocFreq']\n",
    "    \n",
    "    # sort the document on the count\n",
    "    ranked = sorted(ranked.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    return ranked\n",
    "    \n",
    "booleanSearchSumTF([\"love\",\"lightly\"], NewIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "sF1ah2Blh4ce",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a80af5da76db5fb850d3de2c04d78115",
     "grade": true,
     "grade_id": "cell-83ff086299885d29",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(booleanSearchSumTF([\"love\",\"lightly\"], NewIndex)), list)\n",
    "assert_equal(type(booleanSearchSumTF([\"love\",\"lightly\"], NewIndex)[0]), tuple)\n",
    "assert_equal(type(booleanSearchSumTF([\"love\",\"lightly\"], NewIndex)[0][0]), str)\n",
    "assert_equal(type(booleanSearchSumTF([\"love\",\"lightly\"], NewIndex)[0][1]), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "fXPOIhc1h4ce",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5e308b94522529d4dd3edce39569d991",
     "grade": false,
     "grade_id": "cell-cf43362d50d798f4",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "#### 8.3.3 Combined\n",
    "Now combine the two rankings. First rank on number of matching search terms, and then on the sum of the term counts. Return both scores for each DocID.\n",
    "\n",
    "The tuple will thus look like this: `(document ID, number of matching search terms, sum of the term frequency counts)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "yc_UxgIXh4ce",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3319353f4ad9ad9cf795b2b6b34aa3eb",
     "grade": false,
     "grade_id": "cell-13a89b3b12fd9a1d",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def booleanSearchCombined(query, index):\n",
    "    \"\"\"\n",
    "    This function takes a query as input and returns all document ids and\n",
    "    scores of documents which contain at least one of the words. Documents\n",
    "    are ranked first on the number of matching search terms, and then on\n",
    "    the sum of the term frequency counts.\n",
    "    \"\"\"\n",
    "    \n",
    "    ranked = []\n",
    "    \n",
    "    # used to keep track of the location of a document in the ranked list\n",
    "    in_list = {}\n",
    "    counter = 0\n",
    "    \n",
    "    # add the documents with their Doc frequency's to a dict\n",
    "    for word in query:\n",
    "        for document in index[word]['Freq_per_Doc']:\n",
    "            # if document was already in the ranked list\n",
    "            if document in in_list:\n",
    "                # increment its values\n",
    "                ranked[in_list[document]][1] += 1\n",
    "                ranked[in_list[document]][2] += index[word]['DocFreq']\n",
    "            # else we add it to the ranked list\n",
    "            else:\n",
    "                in_list[document] = counter\n",
    "                counter += 1\n",
    "            \n",
    "                ranked.append([document, 1, index[word]['DocFreq']])\n",
    "                \n",
    " \n",
    "    # sort the document (the minus one before x[1] and x[2] are needed to reverse the sorting of the count\n",
    "    # but to keep the alphabetic sorting order)\n",
    "    ranked = sorted(ranked, key=lambda x: (-x[1],-x[2], x[0]))\n",
    "    \n",
    "    # convert to tuple\n",
    "    for i in range(len(ranked)):\n",
    "        ranked[i] = tuple(ranked[i])\n",
    "\n",
    "    return ranked   \n",
    "    \n",
    "booleanSearchCombined([\"love\",\"lightly\"], NewIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "A90RoN8Hh4ce",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a4dbf860bf63192175be4410ac9a5a10",
     "grade": true,
     "grade_id": "cell-6cdd92cfcc2d415d",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(booleanSearchCombined([\"love\",\"lightly\"], NewIndex)), list)\n",
    "assert_equal(type(booleanSearchCombined([\"love\",\"lightly\"], NewIndex)[0]), tuple)\n",
    "assert_equal(type(booleanSearchCombined([\"love\",\"lightly\"], NewIndex)[0][0]), str)\n",
    "assert_equal(type(booleanSearchCombined([\"love\",\"lightly\"], NewIndex)[0][1]), int)\n",
    "assert_equal(type(booleanSearchCombined([\"love\",\"lightly\"], NewIndex)[0][2]), int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "2P52rBOxh4ce",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "a1b5fa82594c281909ab3d4afa876e0b",
     "grade": false,
     "grade_id": "cell-6869c9c2ffc2bfe6",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.4 Boolean AND search\n",
    "Create the function `booleanSearchAND(query, index)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "Ug98zs_Kh4ce",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3d21bc275f29b486de43b3f46a264f69",
     "grade": false,
     "grade_id": "cell-e507e9a68502f0e8",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def booleanSearchAND(query, index):\n",
    "    \"\"\"\n",
    "    This function takes a query as input and returns all document ids as set which\n",
    "    contain ALL the words.\n",
    "    \"\"\"\n",
    "    \n",
    "    sets = []\n",
    "    \n",
    "    # iterate over all the words in the query\n",
    "    for word in query:\n",
    "        # create a set for this particular word\n",
    "        word_set = set()\n",
    "        # add al the documents in which this word occurs to its set\n",
    "        for document in index[word]['Freq_per_Doc']:\n",
    "            word_set.add(document)\n",
    "        # add the set of documents for this particular word to the total list of sets\n",
    "        sets.append(word_set)\n",
    "  \n",
    "    # return the intersection of all the sets in the list of sets\n",
    "    return sets[0].intersection(*sets)\n",
    "    \n",
    "booleanSearchAND([\"lightly\", \"love\"], NewIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "tOkz1T_ah4cf",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "8e661e09840fa0bca11e60218a4b580f",
     "grade": true,
     "grade_id": "cell-ab54ede8ff89e3c7",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "assert_equal(type(booleanSearchAND([\"lightly\", \"love\"], NewIndex)), set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "NFNYH3CGh4cf",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "e9c40b67c0e2916debd7f070e8485025",
     "grade": false,
     "grade_id": "cell-663ad1e3efc39c50",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.5 Ranked Boolean AND search (not graded)\n",
    "How would you implement a ranking for the Boolean AND?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I9U2DOEVh4cf",
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-c34f82add5a898ad",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "You can implement it for the biggest part like the normal Boolean AND search. But to rank all the documents, it is possible to sum the number of every time that a word of the query is in the document. So the different words in a query will count for the ranking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "eUGp0Hqah4cf",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "ca9b372aad0d9b42e09d4d1ee039f82f",
     "grade": false,
     "grade_id": "cell-e21e2af58387577d",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "### 8.6 Relaxed boolean AND search (not graded)\n",
    "How can you implement a \"relaxed version\" of Boolean AND, in which you also give results if not ALL query words are present, but as much as possible? Of course you want to rank the documents again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ao465--Ks6l_"
   },
   "source": [
    "To implement relaxed boolean AND search it is possible to do this to store different information about the documents. Make a dict with the documents as keys and the values represent a list with 2 items. The first item is the amount of words of the query are in the document and the second item is the sum of all words that occures in the document. Loop trough the documents and add a document to the dict if a word of the query exists. Last, rank the documents first by the number of words of the query that are in the document and second by the sum of all words that occures in the document.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "S66zgQaAh4cf",
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c805a9bcaed959fcf90ee77ab9512a07",
     "grade": false,
     "grade_id": "cell-f36e09ed69f4e324",
     "locked": true,
     "points": 1,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "# SQL and full text search\n",
    " \n",
    " Many SQL database systems allow full text search on columns storing text. Unlike with SQL the functionality is not that much standardized but it follows similar ideas. \n",
    " \n",
    " In this exercise you create a bollean search engine, possibly with ranking, in which you can combine full text search with restrictions on other fields. The database used is a collection of transcripts of TedX talks.\n",
    " \n",
    "See the notebook `SQLLiteFullText.ipynb` in this folder for code to get you started and the full description of th exercise. \n",
    "\n",
    "Simply paste your answer to this question here. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "gIPfo2WOh4cV",
    "NFNYH3CGh4cf"
   ],
   "name": "AssignmentWeek1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
